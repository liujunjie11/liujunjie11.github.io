<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LXiHa`Notes</title>
  
  <subtitle>The House Belong to Love and Freedom.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://liujunjie11.github.io/"/>
  <updated>2018-04-13T12:00:46.995Z</updated>
  <id>https://liujunjie11.github.io/</id>
  
  <author>
    <name>刘俊</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>关于在eclipse中运行scrapy项目</title>
    <link href="https://liujunjie11.github.io/2018/04/13/%E5%85%B3%E4%BA%8E%E5%9C%A8eclipse%E4%B8%AD%E8%BF%90%E8%A1%8Cscrapy%E9%A1%B9%E7%9B%AE/"/>
    <id>https://liujunjie11.github.io/2018/04/13/关于在eclipse中运行scrapy项目/</id>
    <published>2018-04-13T11:23:05.000Z</published>
    <updated>2018-04-13T12:00:46.995Z</updated>
    
    <content type="html"><![CDATA[<p>关于在<em>eclipse</em>中运行爬虫<em>scrapy</em>框架的项目介绍如下。</p><blockquote><p><strong>scrapy官方文档：<a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/shell.html" target="_blank" rel="external">http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/shell.html</a></strong></p></blockquote><p>首先打开终端将<code>cd</code>至<em>eclipse</em>目录下的（即<em>eclipse-workspace</em>），使用命令行<code>scrapy startproject tutorial</code>（其中的<em>tutorial</em>是自由选择的），之后会生成一个目录，将其目录移至一个<em>python工程</em>下。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-13%20%E4%B8%8B%E5%8D%887.49.25.png" alt=""></p><blockquote><p>如上图。</p></blockquote><p>在<em>spider</em>包下建立一个脚本文件（如上图我的那个<em>first</em>文件，名字随意），是用来写爬虫程序用的。之后在<em>tutorial</em>包下建立一个名为<em>cmdline</em>的脚本文件（名字随意）。</p><p>下面为了直接达到运行成功的目的，贴上测试用的代码。</p><p><strong>first.py中的代码：</strong></p><pre><code>import scrapyclass DmozSpider(scrapy.Spider):    name = &quot;first&quot; #此处很重要    allowed_domains = [&quot;dmoz.org&quot;]    start_urls = [        &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;,        &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&quot;    ]    def parse(self, response):        filename = response.url.split(&quot;/&quot;)[-2]        with open(filename, &apos;wb&apos;) as f:            f.write(response.body)</code></pre><p><strong>cmdline.py中的代码：</strong></p><pre><code>from scrapy.cmdline import executeif __name__ == &apos;__main__&apos;:    #第三个参数就是上方的first文件名！其他两个参数如下正常    execute(argv=[&apos;scrapy&apos;, &apos;crawl&apos;, &apos;first&apos;])</code></pre><p><strong>items.py中的代码：</strong></p><pre><code>import scrapyclass TutorialItem(scrapy.Item):    # define the fields for your item here like:    # name = scrapy.Field()    title = scrapy.Field()    link = scrapy.Field()    desc = scrapy.Field()</code></pre><blockquote><p>这个文件是与<em>first</em>文件有直接联系的。可参考官方文档介绍。</p></blockquote><h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><p>接下来直接运行<em>cmdline.py</em>可得出结果：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-13%20%E4%B8%8B%E5%8D%887.50.00.png" alt=""></p><hr><h2 id="配置调试"><a href="#配置调试" class="headerlink" title="配置调试"></a>配置调试</h2><p>直接看下面的图上解说吧。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-13%20%E4%B8%8B%E5%8D%887.46.09.png" alt=""></p><hr><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-13%20%E4%B8%8B%E5%8D%887.46.17.png" alt=""></p><blockquote><p>接下来点击debug。</p></blockquote><hr><p><strong>最终两者均可达到输出的效果！</strong></p><p>参考：</p><p><a href="https://blog.csdn.net/otengyue/article/details/48065841" target="_blank" rel="external">https://blog.csdn.net/otengyue/article/details/48065841</a></p><p><a href="http://www.cnblogs.com/v-BigdoG-v/p/7393601.html" target="_blank" rel="external">http://www.cnblogs.com/v-BigdoG-v/p/7393601.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;关于在&lt;em&gt;eclipse&lt;/em&gt;中运行爬虫&lt;em&gt;scrapy&lt;/em&gt;框架的项目介绍如下。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;scrapy官方文档：&lt;a href=&quot;http://scrapy-chs.readthedocs.io/zh_CN/
      
    
    </summary>
    
      <category term="eclispe scrapy" scheme="https://liujunjie11.github.io/categories/eclispe-scrapy/"/>
    
    
      <category term="eclispe scrapy" scheme="https://liujunjie11.github.io/tags/eclispe-scrapy/"/>
    
  </entry>
  
  <entry>
    <title>关于eclipse的风格与主题</title>
    <link href="https://liujunjie11.github.io/2018/04/13/%E5%85%B3%E4%BA%8Eeclipse%E7%9A%84%E9%A3%8E%E6%A0%BC%E4%B8%8E%E4%B8%BB%E9%A2%98/"/>
    <id>https://liujunjie11.github.io/2018/04/13/关于eclipse的风格与主题/</id>
    <published>2018-04-13T04:56:03.000Z</published>
    <updated>2018-04-13T05:21:26.770Z</updated>
    
    <content type="html"><![CDATA[<h2 id="eclipse的界面主题更换"><a href="#eclipse的界面主题更换" class="headerlink" title="eclipse的界面主题更换"></a><em>eclipse</em>的界面主题更换</h2><p>关于我的<em>eclipse</em>的界面主题更换（目前为<em>eclipse Oxygen</em>版本），在内置中已经可以作为一种选择了。</p><p>具体看图所示即可：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/4%E6%9C%88-13-2018%2013-07-55.gif" alt=""></p><blockquote><p>可供的选择有三种，其中的<em>dark</em>类型是比较护眼的，均可试试。</p></blockquote><p>实际的<em>dark</em>展示：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-13%20%E4%B8%8B%E5%8D%881.08.27.png" alt=""></p><hr><h2 id="eclipse代码风格更换"><a href="#eclipse代码风格更换" class="headerlink" title="eclipse代码风格更换"></a><em>eclipse</em>代码风格更换</h2><p>可参考此处：<a href="https://blog.csdn.net/zhouchangshi/article/details/37901519" target="_blank" rel="external">https://blog.csdn.net/zhouchangshi/article/details/37901519</a></p><blockquote><p>其中已经有了许多的并且详细的介绍了。</p></blockquote><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul><li>关于更换编译处的字体大小以及字体选择（如图）:</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/4%E6%9C%88-13-2018%2013-11-02.gif" alt=""></p><ul><li>关于控制台的字体更改以及选择（如图）:</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/4%E6%9C%88-13-2018%2013-10-41.gif" alt=""></p><hr><p><strong>参考：</strong></p><p><a href="https://jingyan.baidu.com/article/f96699bb9442f3894e3c1b15.html" target="_blank" rel="external">https://jingyan.baidu.com/article/f96699bb9442f3894e3c1b15.html</a></p><p><a href="https://blog.csdn.net/zhouchangshi/article/details/37901519" target="_blank" rel="external">https://blog.csdn.net/zhouchangshi/article/details/37901519</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;eclipse的界面主题更换&quot;&gt;&lt;a href=&quot;#eclipse的界面主题更换&quot; class=&quot;headerlink&quot; title=&quot;eclipse的界面主题更换&quot;&gt;&lt;/a&gt;&lt;em&gt;eclipse&lt;/em&gt;的界面主题更换&lt;/h2&gt;&lt;p&gt;关于我的&lt;em&gt;eclip
      
    
    </summary>
    
      <category term="eclipse" scheme="https://liujunjie11.github.io/categories/eclipse/"/>
    
    
      <category term="eclipse" scheme="https://liujunjie11.github.io/tags/eclipse/"/>
    
  </entry>
  
  <entry>
    <title>python3爬虫：爬取淘宝商品信息的两种方法</title>
    <link href="https://liujunjie11.github.io/2018/04/08/python3%E7%88%AC%E8%99%AB%EF%BC%9A%E7%88%AC%E5%8F%96%E6%B7%98%E5%AE%9D%E5%95%86%E5%93%81%E4%BF%A1%E6%81%AF%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95/"/>
    <id>https://liujunjie11.github.io/2018/04/08/python3爬虫：爬取淘宝商品信息的两种方法/</id>
    <published>2018-04-08T10:58:00.000Z</published>
    <updated>2018-04-09T03:59:21.431Z</updated>
    
    <content type="html"><![CDATA[<p>爬取淘宝信息：</p><blockquote><p>1，可用<em>selenium</em>模块与<em>driver Chrome</em>插件自动化模拟爬取。</p><p>2， 进行抓包爬取。</p></blockquote><p><strong>下面一一进行介绍并且贴出实现代码。但是在此必须说明一下，因为时间有限，相关模块不懂的还需要自行搜索，在此不做教程解说，只提供分析思路与代码实现过程。</strong></p><blockquote><p>学习博客推荐：<a href="https://cuiqingcai.com，" target="_blank" rel="external">https://cuiqingcai.com，</a><br>实际上我也是根据此博客学习到了一些知识，也是我参考学习的唯一博客。</p></blockquote><h2 id="自动化模拟爬取"><a href="#自动化模拟爬取" class="headerlink" title="自动化模拟爬取"></a>自动化模拟爬取</h2><p>python爬虫的<em>selenium</em>模块是一个可以实现模拟浏览器相关动作，并且可跳过<em>JavaScript动态渲染</em>直接返回其网页源代码的一个自动化测试工具。如下我们需要爬取关键词为<em>MacBook Pro</em>的商品信息，可用它来实现自动翻页并且返回每一页的源代码直接进行相关信息的爬取。</p><h3 id="分析过程"><a href="#分析过程" class="headerlink" title="分析过程"></a>分析过程</h3><p>下面直接进行分析过程。</p><p>首先打开淘宝主页：<a href="https://www.taobao.com" target="_blank" rel="external">https://www.taobao.com</a> ，进入其中输入关键词观察一下页面，可以看到下面翻页的一些跳转功能。（以下使用的都是<em>Chrome</em>自带的<em>开发者工具</em>）</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-08%20%E4%B8%8B%E5%8D%888.10.59.png" alt=""></p><p>因为要实现自动翻页的过程，则我们需要查看有关翻页这个功能按钮的源代码是什么样子。其中涉及到了输入框，然后点击旁边的确定按钮。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-08%20%E4%B8%8B%E5%8D%888.15.27.png" alt=""></p><blockquote><p>输入框对应的源代码。</p></blockquote><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-08%20%E4%B8%8B%E5%8D%888.18.39.png" alt=""></p><blockquote><p>确定按钮对应的源代码。</p></blockquote><p>在知道了以上的一些按钮信息之后我们还需要知道翻页之后目前对应的页数在哪，以便在对应的页数正确之后，可以得到对应页面的源代码，这样可直接运用一些相关的模块进行节点锁定爬取即可。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-08%20%E4%B8%8B%E5%8D%888.19.13.png" alt=""></p><blockquote><p>确定页数码的源代码。</p></blockquote><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-08%20%E4%B8%8B%E5%8D%887.32.20.png" alt=""></p><blockquote><p>页面上商品信息的html源代码。</p></blockquote><p>为什么要这样分析每一个相关功能的源代码？<strong>因为在selenium模块中可以实现自动化的过程，但是需要指定页面上相关的源代码才行，对于selenium来说等于开放了这个功能的接口一样。</strong></p><p><strong>思路分析：将要用相关模块挖掘源代码信息做为一个函数，将要爬取的页面源代码做为一个函数，其中实现自动翻页功能，即若是大于1，自动跳到其页面并且返回其页源代码，否则返回第一页页面的源代码。最后代入前者挖掘的函数。</strong></p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><pre><code>&apos;&apos;&apos;    函数目标：    利用selenium的自动翻页进行爬去相关的内容    编写时间：    2018-04-07&apos;&apos;&apos;from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.webdriver.support.wait import WebDriverWaitfrom urllib.parse import quotefrom pyquery import PyQuery as pqbrowser = webdriver.Chrome(&apos;/Users/junjieliu/Downloads/小插件/chromedriver&apos;)wait = WebDriverWait(browser, 10)keys = &apos;macbookpro&apos;&apos;&apos;&apos;    函数说明：实现自动化翻页。    仅仅当需要时进行翻页的选项    &apos;&apos;&apos;def index_source(page):    print(&apos;正在爬取第&apos; + str(page) + &apos;页内容..&apos; + &apos;\n&apos; + &apos;.&apos; * 15)    url = &apos;https://s.taobao.com/search?q=&apos; + quote(keys)    browser.get(url)    # 果然页数大于1则自动翻页功能启动    if page &gt; 1:        # 输入框定位        input = wait.until(            EC.presence_of_element_located((By.CSS_SELECTOR, &apos;#mainsrp-pager div.form &gt; input.input.J_Input&apos;)))        # 确定按钮定位        button = wait.until(            EC.element_to_be_clickable((By.CSS_SELECTOR, &apos;#mainsrp-pager div.form &gt; span.btn.J_Submit&apos;)))        # 清理，输入，点击一体化        input.clear()        input.send_keys(page)        button.click()    &apos;&apos;&apos;    获取源代码，传入爬取数据的函数    获取源代码过程：    第一步确定为在那一页    第二步确定爬取内容    &apos;&apos;&apos;    wait.until(        EC.text_to_be_present_in_element((By.CSS_SELECTOR, &apos;#mainsrp-pager li.item.active &gt; span.num&apos;), str(page)))    wait.until(        EC.presence_of_element_located((By.CSS_SELECTOR, &apos;.m-itemlist .items .item&apos;)))    print(&apos;获取本页源码成功，以下为相关的信息：&apos; + &apos;\n&apos;)    get_product()&apos;&apos;&apos;    函数说明：    用pyquery爬取其中的商品内容    &apos;&apos;&apos;def get_product():    html_source = browser.page_source    doc = pq(html_source)    items = doc(&apos;#mainsrp-itemlist .items .item&apos;).items()    &apos;&apos;&apos;        以下匹配内容可查看网页源代码可知        &apos;&apos;&apos;    for item in items:        product_infos = {            &apos;img_url&apos;:&apos;https:&apos; + item.find(&apos;.pic .img&apos;).attr(&apos;data-src&apos;),  # 店铺主图片地址            &apos;product_desc&apos;:item.find(&apos;.pic .img&apos;).attr(&apos;alt&apos;),  # 商品描述            &apos;price&apos;:item.find(&apos;.price&apos;).text().replace(&apos;\n&apos;, &apos;&apos;),  # 价格            &apos;people&apos;:item.find(&apos;.deal-cnt&apos;).text(),  # 购买人数            &apos;shop_url&apos;:&apos;https:&apos; + item.find(&apos;.shop .shopname&apos;).attr(&apos;href&apos;),  # 店铺信息            &apos;shop_name&apos;:item.find(&apos;.shop&apos;).text(),  # 店铺名称            &apos;shop_location&apos;:item.find(&apos;.location&apos;).text()  # 店铺所在地            }        print(product_infos)if __name__ == &apos;__main__&apos;:    pages = 100  # 根据页面分析可知总共有100页的信息    for num in range(1, pages + 1):        index_source(num)     </code></pre><blockquote><p>当然还可以进行正则表达式的匹配来完成爬取，因为需要时间来测试相关的表达式，所以推荐用<em>pyquery</em>这个库来完成爬取工作，相比于使用过的<em>beautiful</em>模块以及正则表达式，我感觉轻松了不少。</p></blockquote><h2 id="抓包爬取"><a href="#抓包爬取" class="headerlink" title="抓包爬取"></a>抓包爬取</h2><p>以下的抓包是利用的<em>Chrome</em>浏览器完成的，在这之前说了，因为时间关系，关于抓包是什么就不在此做详细介绍了…相信你看了我下面的分析过程会隐约明白一点。</p><h3 id="分析过程-1"><a href="#分析过程-1" class="headerlink" title="分析过程"></a>分析过程</h3><p>打开淘宝主页，输入关键词，在第一页上打开<em>Chrome</em>自带的<em>开发者工具</em>，这样就可以看到第一页的源代码信息了。以下图过程可以看到我们在第一页抓到的相关的<em>淘宝API</em>的数据包。里面有相关的商品信息，均为<em>json</em>格式。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-08%20%E4%B8%8B%E5%8D%887.55.32.png" alt=""></p><hr><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-08%20%E4%B8%8B%E5%8D%887.56.39.png" alt=""></p><blockquote><p>其中的<em>XHR格式</em>不懂可参考此篇文章：<a href="https://developer.mozilla.org/zh-CN/docs/Web/API/XMLHttpRequest" target="_blank" rel="external">https://developer.mozilla.org/zh-CN/docs/Web/API/XMLHttpRequest</a></p></blockquote><p>浏览器新建一个标签页，打开上面的那个地址看看有什么。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-08%20%E4%B8%8B%E5%8D%888.24.11.png" alt=""></p><p><strong>经过一系列的测试发现了其中的那个参数<em>bcoffset=</em>是跳转的决定点，更改其值可得到不同的信息。</strong>这样我们进行思路分析就一目了然了。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-08%20%E4%B8%8B%E5%8D%888.21.47.png" alt=""></p><p><strong>思路分析：传入上面的说的API的URL，根据上面的那个参数的值更改，返回其中的文档信息，即可爬取到商品信息。</strong></p><h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><pre><code>&apos;&apos;&apos;    函数说明：    利用抓包返回的信息直接爬取商品信息    编写时间：    2018-04-08&apos;&apos;&apos;import requestsimport reif __name__ == &apos;__main__&apos;:    # 设定爬取100页，实际上还要多...    pages = 100    for num in range(1, pages + 1):        print(&apos;开始爬取第&apos; + str(num) + &apos;页内容...&apos; + &apos;\n&apos; + &apos;.&apos; * 10)        url = &apos;https://s.taobao.com/api?_ksTS=1523191565870_226&amp;callback=jsonp227&amp;ajax=true&amp;m=customized&amp;stats_click=search_radio_all:1&amp;q=macbookpro&amp;p4ppushleft=1,48&amp;ntoffset=4&amp;s=36&amp;imgfile=&amp;initiative_id=staobaoz_20180408&amp;bcoffset=&apos; + str(num) + &apos;&amp;js=1&amp;ie=utf8&amp;rn=5bd3f39c2ca57f21abe4db8ca60ee49f&apos;        # 代理信息        header = {                &apos;cookie&apos;: &apos;t=da97f7b09403e3340cd6d50780a9385e; cna=PoshExqPgw0CAQG9KdLV29qM; hng=CN%7Czh-CN%7CCNY%7C156; thw=cn; enc=MuF%2FWOzFnau5EcbgcPck31M%2FMhER8txA4ZnjdZI2Dt8xzHdAr%2FN4f0OsOD%2FUcrNpOZWIfEG0GSbPQ8C4U%2BU70A%3D%3D; mt=ci%3D-1_1; cookie2=1f17bf1dfa8b6cf01c17d0fe7306c672; v=0; _tb_token_=eaefe3a15f0bf; JSESSIONID=0BC0D7637EFFD50CA56F8A8F4E451813; isg=BD09zFTCO5bo0p_z6P-VaFHGTJ_3cnEiA2cTwf-D2RRsNl5oySop_Xqg5Wpwlonk&apos;,                &apos;referer&apos;: &apos;https://s.taobao.com/search?q=macbookpro&amp;imgfile=&amp;js=1&amp;stats_click=search_radio_all%3A1&amp;initiative_id=staobaoz_20180408&amp;ie=utf8&amp;bcoffset=4&amp;p4ppushleft=1%2C48&amp;ntoffset=4&amp;s=0&apos;,                &apos;user-agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36&apos;,                &apos;x-requested-with&apos;: &apos;XMLHttpRequest&apos;}        responed = requests.get(url=url, headers=header)        # 指定编码格式        responed.encoding = &apos;utf-8&apos;        re_text = responed.text        &apos;&apos;&apos;            进入数据清洗与筛选阶段            因为转化json格式始终失败...所以改用正则表达式来进行匹配        &apos;&apos;&apos;        print(&apos;获取源代码成功，以下为相关商品的信息：&apos;)        target = re.findall(r&apos;&quot;raw_title&quot;:&quot;(.*?)&quot;.*?&quot;pic_url&quot;:&quot;(.*?)&quot;.*?&quot;view_price&quot;:&quot;(.*?)&quot;.*?&quot;item_loc&quot;:&quot;(.*?)&quot;.*?&quot;view_sales&quot;:&quot;(.*?)&quot;.*?&quot;user_id&quot;:&quot;(.*?)&quot;.*?&quot;nick&quot;:&quot;(.*?)&quot;&apos;, re_text, re.S)  # @UndefinedVariable        for each in target:            product = {                &apos;shop_title&apos;:each[0],  # 店铺主题                &apos;pic_url&apos;:&apos;https:&apos; + each[1],  # 店铺图片地址                &apos;price&apos;:each[2],  # 价格                &apos;sales_people&apos;:each[4],  # 购买人数                &apos;shop_name&apos;:each[6],  # 店铺名称                &apos;loc&apos;:each[3],  # 店铺所在地                &apos;shop_url&apos;:&apos;https://store.taobao.com/shop/view_shop.htm?user_number_id=&apos; + each[5]  # 店铺URL地址                    }            print(product)</code></pre><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>第二种方法期间原本以为可以先转化为<em>json格式</em>来进行处理，没想到发生了<strong>json.decoder.JSONDecodeError:</strong>的错误，结果改用正则表达式进行文本的挖掘，效率也是无形中得到了提升…在此我推荐用正则表达式来处理<em>json格式</em>的文档，这样在绝大多数情况下可能会有更高的效率！</p><p>爬取到的数据可用来进行一些数据分析等等。有时想尝试用各种方法去爬取，但是因为学的东西真的多，所以就不一一尝试了，尽量使用简便的方法…不懂的朋友还需要多多学习，利用好搜索引擎！当然看书也是必须的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;爬取淘宝信息：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;1，可用&lt;em&gt;selenium&lt;/em&gt;模块与&lt;em&gt;driver Chrome&lt;/em&gt;插件自动化模拟爬取。&lt;/p&gt;
&lt;p&gt;2， 进行抓包爬取。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;下面一一进
      
    
    </summary>
    
      <category term="python爬虫" scheme="https://liujunjie11.github.io/categories/python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="python爬虫" scheme="https://liujunjie11.github.io/tags/python%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>关于记录Chromedriver的selenium.common.exceptions.WebDriverException: Message: &#39;chromedriver &#39; executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home</title>
    <link href="https://liujunjie11.github.io/2018/04/05/%E5%85%B3%E4%BA%8E%E8%AE%B0%E5%BD%95Chromedriver%E7%9A%84selenium-common-exceptions-WebDriverException-Message-chromedriver-executable-needs-to-be-in-PATH-Please-see-https-sites-google-com-a-chromium-org-chromedriver-home/"/>
    <id>https://liujunjie11.github.io/2018/04/05/关于记录Chromedriver的selenium-common-exceptions-WebDriverException-Message-chromedriver-executable-needs-to-be-in-PATH-Please-see-https-sites-google-com-a-chromium-org-chromedriver-home/</id>
    <published>2018-04-05T11:46:15.000Z</published>
    <updated>2018-04-05T12:01:34.821Z</updated>
    
    <content type="html"><![CDATA[<p>最近学习一些<em>python</em>爬虫的框架，用<em>selenium</em>与<em>Chromedriver</em>插件结合<em>Chrome浏览器</em>来爬去网页源数据。</p><p>发现了如题的错误信息：</p><blockquote><p>selenium.common.exceptions.WebDriverException: Message:<br>  ‘chromedriver ‘ executable needs to be in PATH. Please see<br>  <a href="https://sites.google.com/a/chromium.org/chromedriver/home" target="_blank" rel="external">https://sites.google.com/a/chromium.org/chromedriver/home</a></p></blockquote><p>在已配置好系统环境的前提下，用如下代码指定位置，或者是将<em>Chromedriver</em>插件放在工程文件下，均得到了如上的错误信息。</p><pre><code>from selenium import webdriverbrowser = webdriver.Chrome(&apos;/Users/junjieliu/Downloads/小插件/chromedriver&apos;)</code></pre><hr><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>如下操作，先将插件打开（即表示打开运行了，<strong>在需要使用时应当一直开着</strong>），跳出来了其所在位置，将其位置复制再运行上面的代码，发现跳出来了一个<em>Chrome浏览器</em>的空白页面，这表示已经成功了！</p><p><img src="http://owudg3xs2.bkt.clouddn.com/4%E6%9C%88-05-2018%2019-56-36.gif" alt=""></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-05%20%E4%B8%8B%E5%8D%888.00.03.png" alt=""></p><blockquote><p>成功的页面。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近学习一些&lt;em&gt;python&lt;/em&gt;爬虫的框架，用&lt;em&gt;selenium&lt;/em&gt;与&lt;em&gt;Chromedriver&lt;/em&gt;插件结合&lt;em&gt;Chrome浏览器&lt;/em&gt;来爬去网页源数据。&lt;/p&gt;
&lt;p&gt;发现了如题的错误信息：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p
      
    
    </summary>
    
      <category term="爬虫" scheme="https://liujunjie11.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="https://liujunjie11.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>python3爬取豆瓣Top250电影信息</title>
    <link href="https://liujunjie11.github.io/2018/04/03/python3%E7%88%AC%E5%8F%96%E8%B1%86%E7%93%A3Top250%E7%94%B5%E5%BD%B1%E4%BF%A1%E6%81%AF/"/>
    <id>https://liujunjie11.github.io/2018/04/03/python3爬取豆瓣Top250电影信息/</id>
    <published>2018-04-03T14:09:36.000Z</published>
    <updated>2018-04-04T02:29:05.477Z</updated>
    
    <content type="html"><![CDATA[<p>以下是通过正则表达式爬取的<em>猫眼电影</em>以及<em>豆瓣电影</em>的相关的电影信息的代码过程。</p><hr><h2 id="爬取猫眼电影"><a href="#爬取猫眼电影" class="headerlink" title="爬取猫眼电影"></a>爬取猫眼电影</h2><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>打开<em>Chrome</em>的调试工具，可发现相关的信息都在，并且以翻页的形式来得到更多的电影信息。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-04%20%E4%B8%8A%E5%8D%8810.17.59.png" alt=""></p><blockquote><p>我们可通过其爬取需要的信息，利用正则表达式来进行文本的挖掘。</p></blockquote><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><pre><code>&apos;&apos;&apos;    函数目标：    爬取猫眼电影的top100信息    编写时间：    2018-03-28&apos;&apos;&apos;&apos;&apos;&apos;import requestsimport reimport jsonif __name__ == &apos;__main__&apos;:print(&apos;猫眼电影Top100信息如下：&apos;)#循环10次，得出页面上的Top100的电影信息for i in range(0, 10):    url = &quot;http://maoyan.com/board/4?&quot;    header = {            &apos;Host&apos;:&apos;maoyan.com&apos;,            &apos;Referer&apos;:&apos;http://maoyan.com/board/4?offset=20&apos;,            &apos;User-Agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36&apos;}    paramters = {            &apos;offset&apos;: i * 10}    request_tasget = requests.get(url=url, headers=header, params=paramters)    request_tasget.encoding = &apos;utf-8&apos;    # 实际上应该考虑考虑网的问题...连不上就一般没有数据返回了，找一个字符作为接口之后利用万能表达式即可    infos_list = re.findall(    r&apos;&lt;dd&gt;.*?board-index.*?&gt;(.*?)&lt;/i&gt;.*?alt.*?src=&quot;(.*?)&quot;.*?&lt;a.*?&gt;(.*?)&lt;/a&gt;.*?&quot;star&quot;&gt;(.*?)&lt;/p&gt;.*?&quot;releasetime&quot;&gt;(.*?)&lt;/p&gt;.*?&quot;integer&quot;&gt;(.*?)&lt;/i&gt;.*?&quot;fraction&quot;&gt;(.*?)&lt;/i&gt;&apos;,    request_tasget.text, re.S)  # @UndefinedVariable    #建立存储信息的字典    for each in infos_list:        yields = {                &apos;index&apos;:each[0],                &apos;image_info&apos;:each[1],                &apos;name&apos;:each[2],                &apos;actor&apos;:each[3].strip(),                &apos;time&apos;:each[4],                &apos;score&apos;:each[5] + each[6]                }        print(yields)</code></pre><h2 id="爬取豆瓣电影"><a href="#爬取豆瓣电影" class="headerlink" title="爬取豆瓣电影"></a>爬取豆瓣电影</h2><h3 id="分析-1"><a href="#分析-1" class="headerlink" title="分析"></a>分析</h3><p>与爬取猫眼电影信息一样，都是一样的，具体的差距还是在豆瓣上爬取信息需要<em>cookie</em>，具体可参考下面的代码。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-04%20%E4%B8%8A%E5%8D%8810.18.27.png" alt=""></p><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><pre><code>&apos;&apos;&apos;    函数目标：    爬取豆瓣Top250的电影信息    编写时间：    2018-04-01&apos;&apos;&apos;import requestsimport re if __name__ == &apos;__main__&apos;:print(&apos;以下为豆瓣Top250的电影信息：&apos;)# 从页面可看到一共有十页，每页上有25个电影的信息for i in range(0, 10):    url = &quot;https://movie.douban.com/top250?&quot;    header = {            &apos;Connection&apos;: &apos;keep-alive&apos;,            &apos;Cookie&apos;: &apos;bid=x9ipkoKiQgw; _pk_ses.100001.4cf6=*; __utma=30149280.2001666009.1522587991.1522587991.1522587991.1; __utmb=30149280.0.10.1522587991; __utmc=30149280; __utmz=30149280.1522587991.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); __utma=223695111.1530785565.1522587991.1522587991.1522587991.1; __utmb=223695111.0.10.1522587991; __utmc=223695111; __utmz=223695111.1522587991.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); ll=&quot;118146&quot;; _vwo_uuid_v2=D575771A914BC38B3B7D081C0A0296FDC|939d8f7fdff1f383a97d572dab39fa1a; _pk_id.100001.4cf6=1de732c7ac4dc22b.1522587990.1.1522588516.1522587990.; ct=y&apos;,            &apos;Host&apos;: &apos;movie.douban.com&apos;,            &apos;Referer&apos;: &apos;https://movie.douban.com/top250?start=25&amp;filter=&apos;,            &apos;Upgrade-Insecure-Requests&apos;: &apos;1&apos;,            &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36}&apos;             }    paramters = {             &apos;start&apos;: i * 25,             &apos;filter&apos;:&apos;&apos;              }    tasget = requests.get(url=url, headers=header, params=paramters)    tasget.encoding = &apos;utf-8&apos;    # 匹配正则表达式    infos = re.findall(r&apos;&lt;li&gt;.*?&lt;em class=&quot;&quot;&gt;(.*?)&lt;/em&gt;.*?alt=.*?src=&quot;(.*?)&quot;.*?&quot;title&quot;&gt;(.*?)&lt;/span&gt;.*?class=&quot;title&quot;&gt;&amp;nbsp;/&amp;nbsp;(.*?)&lt;/span&gt;.*?&quot;other&quot;&gt;&amp;nbsp;/&amp;nbsp;(.*?)&lt;/span&gt;.*?&lt;p class=&quot;&quot;&gt;(.*?)&amp;nbsp;&amp;nbsp;&amp;nbsp;(.*?)&lt;br&gt;(.*?)&amp;nbsp;/&amp;nbsp(.*?)&amp;nbsp;/&amp;nbsp(.*?)&lt;/p&gt;.*?average&quot;&gt;(.*?)&lt;/span&gt;&apos;, tasget.text, re.S)  # @UndefinedVariable    # 建立一个空字典用于存储相关的信息    for each in infos:        yields = {            &apos;index&apos;:each[0],  # 排名            &apos;img_info&apos;:each[1],  # 照片地址            &apos;name&apos;:each[2].strip() + &quot;/&quot; + each[3].strip() + &quot;/&quot; + each[4].strip(),  # 影片名称            &apos;director&apos;:each[5].strip(),  # 导演            &apos;actor&apos;:each[6],  # 演员            &apos;time_cy&apos;:each[7].strip() + each[8],  # 上演时间及地区            &apos;type&apos;:each[9].strip(),  # 影片类型            &apos;score&apos;:each[10]  # 评分            }        print(yields)</code></pre><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>简单说说其中的含义，表达式<code>.*?</code>是一个万能的匹配式，<code>(.*?)</code>是匹配想要爬取的内容，并且每一次伴随着一个索引号，每一个索引号对应着的信息不同。利用正则表达式可方便的进行文本与代码的分开挖掘，一般在写正则表达式时用<em>html</em>代码中的一些词来进行过渡就可以方便的写出来了。</p><blockquote><p>多说无益，还是需要自己来进行代码的测试了解的。</p></blockquote><ul><li>参考的博客：<a href="https://cuiqingcai.com" target="_blank" rel="external">https://cuiqingcai.com</a></li></ul><blockquote><p>可根据此博客来学习更多的爬虫知识。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;以下是通过正则表达式爬取的&lt;em&gt;猫眼电影&lt;/em&gt;以及&lt;em&gt;豆瓣电影&lt;/em&gt;的相关的电影信息的代码过程。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;爬取猫眼电影&quot;&gt;&lt;a href=&quot;#爬取猫眼电影&quot; class=&quot;headerlink&quot; title=&quot;爬取猫眼电影&quot;&gt;&lt;/a&gt;
      
    
    </summary>
    
      <category term="python爬虫" scheme="https://liujunjie11.github.io/categories/python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="python3爬虫" scheme="https://liujunjie11.github.io/tags/python3%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Mac下的mysql安装以及相关的问题解决</title>
    <link href="https://liujunjie11.github.io/2018/04/03/Mac%E4%B8%8B%E7%9A%84mysql%E5%AE%89%E8%A3%85%E4%BB%A5%E5%8F%8A%E7%9B%B8%E5%85%B3%E7%9A%84%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/"/>
    <id>https://liujunjie11.github.io/2018/04/03/Mac下的mysql安装以及相关的问题解决/</id>
    <published>2018-04-03T11:48:48.000Z</published>
    <updated>2018-04-03T12:58:55.064Z</updated>
    
    <content type="html"><![CDATA[<p>最近因为学习到了爬虫的原因，存储数据需要<em>mysql</em>了，因为以前学习过其命令行，所以在之前安装过了，太久没用了，怎么开都不懂了，搞了几十分钟不想搞了，重新安装。下面记录下遇到的一些问题。</p><hr><h2 id="安装过程"><a href="#安装过程" class="headerlink" title="安装过程"></a>安装过程</h2><h3 id="第一种方法"><a href="#第一种方法" class="headerlink" title="第一种方法"></a>第一种方法</h3><p>进入官网下载对应<em>Mac</em>的<em>mysql</em>。</p><blockquote><p><a href="https://dev.mysql.com/downloads/mysql/" target="_blank" rel="external">官网地址</a></p></blockquote><h3 id="第二种方法"><a href="#第二种方法" class="headerlink" title="第二种方法"></a>第二种方法</h3><p>直接用命令行<code>brew Install mysql</code>。</p><blockquote><p>前提是必须安装了<em>homebrew</em>。</p></blockquote><h2 id="配置以及开启过程"><a href="#配置以及开启过程" class="headerlink" title="配置以及开启过程"></a>配置以及开启过程</h2><h3 id="配置过程"><a href="#配置过程" class="headerlink" title="配置过程"></a>配置过程</h3><p>在手动安装正后一步记下默认的密码（如下图）。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-03%20%E4%B8%8B%E5%8D%888.34.15.png" alt=""></p><blockquote><p>即<strong>QeV.a&gt;zGa1m3</strong>为默认密码。</p></blockquote><p>在<em>偏好系统</em>中手动打开<em>mysql</em>应用。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-03%20%E4%B8%8B%E5%8D%888.34.36.png" alt=""></p><p>之后打开终端，为其配置。（命令行如下依次）</p><p><code>vi ~/bash_profile</code></p><p>进入按字母<em>i</em>进入编辑模式，输入：</p><p><code>export PATH=&quot;$PATH:/usr/local/mysql/bin&quot;</code></p><blockquote><p>在此可能有疑惑，为什么可以直接<code>mysql/bin</code>？因为在我们手动下载<br><em>mysql</em>之后系统已经自动的复制了一遍，并且将其名为<em>mysql</em>，如图在<em>/usr/local/</em>可找到。</p></blockquote><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-03%20%E4%B8%8B%E5%8D%888.18.02.png" alt=""></p><hr><p>接着输入相关的命令…这时已经配置好了，按下<em>esc</em>健进入命令行模式，输入<code>：wq</code>,推出。之后为了快速见效输入<code>source ~/bash_profile</code></p><h3 id="启动过程"><a href="#启动过程" class="headerlink" title="启动过程"></a>启动过程</h3><p>输入命令行<code>mysql -uroot -p</code>,提示输入刚刚的默认密码，进入到了<em>mysql</em>的编译界面之后，我们修改默认密码，输入代码<code>set PASSWORD =PASSWORD(&#39;123456&#39;);</code></p><blockquote><p><strong>其中的<em>123456</em>为新的密码。</strong></p></blockquote><p>之后为测试是否已经修改成功，输入<code>exit();</code>推出界面，输入命令行<code>mysql -uroot -p</code>,提示输入刚刚的新密码，成功进入编译界面。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-03%20%E4%B8%8B%E5%8D%888.32.00.png" alt=""></p><h2 id="卸载"><a href="#卸载" class="headerlink" title="卸载"></a>卸载</h2><p>可参考：<a href="https://www.jianshu.com/p/b02be6026a2a" target="_blank" rel="external">https://www.jianshu.com/p/b02be6026a2a</a></p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p><strong>解决问题：ERROR 1045 (28000): Access denied for user ‘root’@’localhost’ (using password: YES)</strong></p><p>可参考：<a href="https://www.digitalocean.com/community/questions/setup-mysql-on-ubuntu-droplet-getting-error-error-1045-28000-access-denied-for-user-root-localhost-using-password-yes" target="_blank" rel="external">https://www.digitalocean.com/community/questions/setup-mysql-on-ubuntu-droplet-getting-error-error-1045-28000-access-denied-for-user-root-localhost-using-password-yes</a></p><p><strong>解决进程问题</strong></p><p>可参考：<a href="https://blog.csdn.net/liumaolincycle/article/details/51896592" target="_blank" rel="external">https://blog.csdn.net/liumaolincycle/article/details/51896592</a></p><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>有时下载完之后也会出现上面的那个问题，我是通过重新启动电脑之后获得了解决。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近因为学习到了爬虫的原因，存储数据需要&lt;em&gt;mysql&lt;/em&gt;了，因为以前学习过其命令行，所以在之前安装过了，太久没用了，怎么开都不懂了，搞了几十分钟不想搞了，重新安装。下面记录下遇到的一些问题。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;安装过程&quot;&gt;&lt;a href=&quot;#安装
      
    
    </summary>
    
      <category term="mysql" scheme="https://liujunjie11.github.io/categories/mysql/"/>
    
    
      <category term="mysql" scheme="https://liujunjie11.github.io/tags/mysql/"/>
    
  </entry>
  
  <entry>
    <title>关于解决python中UnicodeDecodeError: &#39;ascii&#39; codec can&#39;t decode byte 0xe8 in position 67986: ordinal not in range(128)的问题</title>
    <link href="https://liujunjie11.github.io/2018/03/28/%E5%85%B3%E4%BA%8E%E8%A7%A3%E5%86%B3python%E4%B8%ADUnicodeDecodeError-ascii-codec-can-t-decode-byte-0xe8-in-position-67986-ordinal-not-in-range-128-%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>https://liujunjie11.github.io/2018/03/28/关于解决python中UnicodeDecodeError-ascii-codec-can-t-decode-byte-0xe8-in-position-67986-ordinal-not-in-range-128-的问题/</id>
    <published>2018-03-28T10:18:54.000Z</published>
    <updated>2018-03-28T10:26:59.245Z</updated>
    
    <content type="html"><![CDATA[<p>在学习爬虫的过程中，在运行编者的代码时出现了<em>UnicodeDecodeError: ‘ascii’ codec can’t decode byte 0xe8 in position 67986: ordinal not in range(128)</em>的编码问题。</p><p>具体原因是因为程序默认的解析编码格式发生了冲突造成的，简单来说就是需要解析的内容与<em>API</em>程序默认的解析编码格式不同，所以才会出现如上的错误。</p><p>因为是打算用<em>python</em>解析<em>JS</em>文件，所以我就贴上我的解决方案了：</p><pre><code>ctx = node.compile(open(file, encoding=&apos;utf-8&apos;).read())</code></pre><blockquote><p>加上自行规定的解码格式即可。</p></blockquote><p><strong>解决来源于GitHub论坛…</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在学习爬虫的过程中，在运行编者的代码时出现了&lt;em&gt;UnicodeDecodeError: ‘ascii’ codec can’t decode byte 0xe8 in position 67986: ordinal not in range(128)&lt;/em&gt;的编码问题
      
    
    </summary>
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>关于homebrew下载出现&quot;Error/go/version missing for  gotools resource!&quot;错误的解决方案</title>
    <link href="https://liujunjie11.github.io/2018/03/28/%E5%85%B3%E4%BA%8Ehomebrew%E4%B8%8B%E8%BD%BD%E5%87%BA%E7%8E%B0Error:go:version%20missing%20for%20%20gotools%20resource!%E9%94%99%E8%AF%AF%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    <id>https://liujunjie11.github.io/2018/03/28/关于homebrew下载出现Error:go:version missing for  gotools resource!错误的解决方案/</id>
    <published>2018-03-28T02:47:34.000Z</published>
    <updated>2018-03-28T02:48:35.030Z</updated>
    
    <content type="html"><![CDATA[<p>最近用<em>homebrew</em>下载<em>mongodb</em>，发现出现了<em>Error: go: version missing for “gotools” resource!</em>的错误，试了几次都不行，最后在谷歌用英文搜索关键词在<em>GitHub</em>的一个论坛用相关的解决方案。</p><p><strong>使用命令行</strong></p><blockquote><p><strong>git -C “$(brew –repo)” fetch –tags</strong></p></blockquote><p>之后再次输入：</p><blockquote><p><strong>brew update –force</strong></p></blockquote><p>最后再次用命令行<em>brew install mongodb</em>，发现已经能够正确成功下载了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近用&lt;em&gt;homebrew&lt;/em&gt;下载&lt;em&gt;mongodb&lt;/em&gt;，发现出现了&lt;em&gt;Error: go: version missing for “gotools” resource!&lt;/em&gt;的错误，试了几次都不行，最后在谷歌用英文搜索关键词在&lt;em&gt;GitH
      
    
    </summary>
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>数据结构与算法：线性表</title>
    <link href="https://liujunjie11.github.io/2018/03/26/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%9A%E7%BA%BF%E6%80%A7%E8%A1%A8/"/>
    <id>https://liujunjie11.github.io/2018/03/26/数据结构与算法：线性表/</id>
    <published>2018-03-26T10:56:19.000Z</published>
    <updated>2018-04-03T12:29:47.274Z</updated>
    
    <content type="html"><![CDATA[<p>打算重新好好系统的学习数据结构了。这是开始。</p><p>因为概念已经是烂大街了，在此只记录下我用<em>java</em>或者是<em>python</em>实现相关功能的代码。</p><hr><h2 id="查找"><a href="#查找" class="headerlink" title="查找"></a>查找</h2><h3 id="java实现"><a href="#java实现" class="headerlink" title="java实现"></a>java实现</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;打算重新好好系统的学习数据结构了。这是开始。&lt;/p&gt;
&lt;p&gt;因为概念已经是烂大街了，在此只记录下我用&lt;em&gt;java&lt;/em&gt;或者是&lt;em&gt;python&lt;/em&gt;实现相关功能的代码。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;查找&quot;&gt;&lt;a href=&quot;#查找&quot; class=&quot;hea
      
    
    </summary>
    
      <category term="数据结构与算法" scheme="https://liujunjie11.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="数据结构与算法" scheme="https://liujunjie11.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>计算机基础知识：认识计算机</title>
    <link href="https://liujunjie11.github.io/2018/03/26/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%9A%E8%AE%A4%E8%AF%86%E8%AE%A1%E7%AE%97%E6%9C%BA/"/>
    <id>https://liujunjie11.github.io/2018/03/26/计算机基础知识：认识计算机/</id>
    <published>2018-03-26T03:44:12.000Z</published>
    <updated>2018-03-26T05:12:33.821Z</updated>
    
    <content type="html"><![CDATA[<p>计算机发展到了如今这样的一个地步实在让人感兴趣…</p><p>于是看了一些书和文章，想了解了解一个计算机是如何跑起来的。</p><hr><h2 id="机器本质"><a href="#机器本质" class="headerlink" title="机器本质"></a>机器本质</h2><blockquote><p><strong>计算机的本质</strong></p></blockquote><ul><li>计算机的本质<strong>：（电脑）计算机 = 计算机器</strong></li></ul><p>没错，如今<strong>我们使用的计算机就是和我们小学时按的那个只会加减乘除计算器的本质毫无区别</strong>。使用计算机时，我们会先通过键盘或者是语音输入想要了解的信息，之后计算机会通过内部一系列的运算之后，输出相关的界面信息到我们的屏幕上。那我们可理解计算机就是一通过我们输入，然后自己默默运算完输出结果的机器。电脑电脑，正所谓脑子就是人类的<strong>计算机器</strong>，电脑不过是插上电源才能运行的计算机器。</p><p><img src="" alt=""></p><p>什么图画啊，文档内容等等均是先转化为相关的数字信息（如我们学过的二进制，十六进制等）后才在显示器上呈现出来的，所以在此可理解为什么还会有那么多各式各样的字符编码了吧。如中文编码常见的<em>GB2312</em>等。</p><blockquote><p><strong>计算机上的1与0</strong></p></blockquote><ul><li><strong>计算机是电子产品，其构造是由一些硬件组成。</strong></li></ul><p>再继续说说烂大街的绝大部分地球人都知道的事实：<strong>计算机只认识1和0</strong>。因为计算机是集成电路（IC）组成,运行起来接上电源之后需要传输数据了（即属于电子数字电路），而在数字电路中，二进制（binary）数是指用二进制记数系统，即以2为基数的记数系统表示的数字。这一系统中，通常用两个不同的符号0（代表零）和1（代表一）来表示。以2为基数代表系统是二进位制的。数字电子电路中，逻辑门的实现直接应用了二进制，因此现代的计算机和依赖计算机的设备里都用到二进制。每个数字称为一个位元（二进制位）或比特（Bit，Binary digit的缩写）。补充：<strong>比特是信息的最小单位， 字节是信息的基本单位。</strong></p><p>结合上面的说明，可知道数据都是在计算机中以1和0组成的，一是因为计算机本身的结构所致，二是因为这样可方便的转化为其他的数字类型（包括十进制之类的）。<strong>对于计算机来说，什么都是数字。只是数字的组成不同而已。</strong></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><p>维基百科：<a href="https://zh.wikipedia.org/wiki/二进制" target="_blank" rel="external">https://zh.wikipedia.org/wiki/二进制</a></p></li><li><p>书籍：《计算机是怎么跑起来的》 矢泽久雄 著</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;计算机发展到了如今这样的一个地步实在让人感兴趣…&lt;/p&gt;
&lt;p&gt;于是看了一些书和文章，想了解了解一个计算机是如何跑起来的。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;机器本质&quot;&gt;&lt;a href=&quot;#机器本质&quot; class=&quot;headerlink&quot; title=&quot;机器本质&quot;&gt;&lt;/a&gt;
      
    
    </summary>
    
      <category term="计算机基础知识" scheme="https://liujunjie11.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="计算机基础知识" scheme="https://liujunjie11.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>Mac下安装Redis以及其可视化客户端</title>
    <link href="https://liujunjie11.github.io/2018/03/22/Mac%E4%B8%8B%E5%AE%89%E8%A3%85Redis%E4%BB%A5%E5%8F%8A%E5%85%B6%E5%8F%AF%E8%A7%86%E5%8C%96%E5%AE%A2%E6%88%B7%E7%AB%AF/"/>
    <id>https://liujunjie11.github.io/2018/03/22/Mac下安装Redis以及其可视化客户端/</id>
    <published>2018-03-22T13:17:30.000Z</published>
    <updated>2018-03-22T13:40:22.174Z</updated>
    
    <content type="html"><![CDATA[<p>最近想要用<em>Nosql</em>结合做一些小项目，用的是<em>Mac</em>，看到网上的教程有点乱七八糟的了，就打算记录下来了。</p><p>关于这个数据库就不在此介绍了。</p><h2 id="安装过程"><a href="#安装过程" class="headerlink" title="安装过程"></a>安装过程</h2><blockquote><p>来到官网页面下载：<a href="https://redis.io/download" target="_blank" rel="external">https://redis.io/download</a></p></blockquote><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-22%20%E4%B8%8B%E5%8D%889.35.10.png" alt=""></p><hr><p>下载好之后解压，并且要知道目录文件在哪。现在打开终端，用命令 <code>cd 文件目录地址</code>，如我的是 <code>cd /Users/junjieliu/Documents/编程文件/redis-4.0.8</code>，之后使用命令 <code>sudo make</code> 成功之后出现如下图1所示，之后再使用命令 <code>make test</code>,成功之后出现如图2所示。</p><ul><li>图1:</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-22%20%E4%B8%8B%E5%8D%888.53.59.png" alt=""></p><ul><li>图2:</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-22%20%E4%B8%8B%E5%8D%888.57.13.png" alt=""></p><p>之后，编译安装：在终端中输入命令：<code>sudo make install</code></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-22%20%E4%B8%8B%E5%8D%888.57.56.png" alt=""></p><p>启动Redis,输入命令<code>redis-server</code></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-22%20%E4%B8%8B%E5%8D%888.58.33.png" alt=""></p><blockquote><p>至此就安装成功了，以上命令有时可能会有出现错误的结果，多试试。</p></blockquote><h2 id="安装Redis-Desktop-Manager"><a href="#安装Redis-Desktop-Manager" class="headerlink" title="安装Redis Desktop Manager"></a>安装Redis Desktop Manager</h2><p>到此网站下载即可：<a href="https://sourceforge.net/projects/redis-desktop-manager.mirror/" target="_blank" rel="external">https://sourceforge.net/projects/redis-desktop-manager.mirror/</a></p><blockquote><p>当然可见我的云盘分享：<a href="https://pan.baidu.com/s/1Bvc7_tZ5yUnnwfnH2bDocg" target="_blank" rel="external">https://pan.baidu.com/s/1Bvc7_tZ5yUnnwfnH2bDocg</a></p></blockquote><p>下载完之后将软件移植系统<em>应用程序</em>一栏，打开：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-22%20%E4%B8%8B%E5%8D%889.10.11.png" alt=""></p><hr><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>介绍好评高的一本电子书《Redis实战》：</p><p><a href="http://www.java1234.com/a/javabook/database/2017/0625/8356.html" target="_blank" rel="external">http://www.java1234.com/a/javabook/database/2017/0625/8356.html</a></p><blockquote><p>进入网站即有相关的资源。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近想要用&lt;em&gt;Nosql&lt;/em&gt;结合做一些小项目，用的是&lt;em&gt;Mac&lt;/em&gt;，看到网上的教程有点乱七八糟的了，就打算记录下来了。&lt;/p&gt;
&lt;p&gt;关于这个数据库就不在此介绍了。&lt;/p&gt;
&lt;h2 id=&quot;安装过程&quot;&gt;&lt;a href=&quot;#安装过程&quot; class=&quot;hea
      
    
    </summary>
    
      <category term="教程" scheme="https://liujunjie11.github.io/categories/%E6%95%99%E7%A8%8B/"/>
    
    
      <category term="教程" scheme="https://liujunjie11.github.io/tags/%E6%95%99%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>用python/R数据分析可视化汽车使用燃料情况</title>
    <link href="https://liujunjie11.github.io/2018/03/21/%E7%94%A8python:R%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%8F%AF%E8%A7%86%E5%8C%96%E6%B1%BD%E8%BD%A6%E4%BD%BF%E7%94%A8%E7%87%83%E6%96%99%E6%83%85%E5%86%B5/"/>
    <id>https://liujunjie11.github.io/2018/03/21/用python:R数据分析可视化汽车使用燃料情况/</id>
    <published>2018-03-21T12:40:06.000Z</published>
    <updated>2018-03-22T07:26:01.239Z</updated>
    
    <content type="html"><![CDATA[<p>分析的是美国一个网站统计的多年汽车使用燃料情况的数据，需要从网上下载，在此利用好<em>python爬虫</em>大有裨益，可作为实战运行分析。</p><blockquote><p>此篇文章案例来源于：<img src="" alt=""></p><p>本书文章中用<em>R语言</em>实现的数据可视化，是从网上直接下的文件然后分析，我打算在此用<em>python</em>实现爬取相关文件，并且运用<em>python</em>进行可视化分析。</p></blockquote><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>进入主页，得到了下载文件的主链接：</p><p><img src="" alt=""></p><p>在此可见资源的下载地址：</p><p><img src="" alt=""></p><hr><blockquote><p>接下来就是运用爬虫知识实现此文件的下载了。</p></blockquote><h2 id="爬虫代码"><a href="#爬虫代码" class="headerlink" title="爬虫代码"></a>爬虫代码</h2><p>我们可有这样的思路：<strong>从主页出发 –&gt; 爬取到下载文件页面的链接 –&gt; 再从此爬取到下载地址链接 –&gt; 之后运用相关的函数下载至本目录即可。</strong>以下是实现过程。</p><pre><code>&apos;&apos;&apos;    函数目标：    用python爬取相关的下载文件    编写时间：    2018-03-21&apos;&apos;&apos;import requestsimport refrom bs4 import BeautifulSoupfrom urllib.request import urlretrieve&apos;&apos;&apos;    first_url方法获取进入下载页面的地址    运用了正则表达式的匹配方法                                        &apos;&apos;&apos;def first_url():    url = &apos;https://www.fueleconomy.gov/&apos;    # 添加代理    header = {&apos;Accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&apos;,            &apos;Accept-Encoding&apos;: &apos;gzip, deflate, br&apos;,            &apos;Accept-Language&apos;: &apos;zh-CN,zh&apos;,            &apos;Cache-Control&apos;: &apos;max-age=0&apos;,            &apos;Connection&apos;: &apos;keep-alive&apos;,            &apos;Host&apos;: &apos;www.fueleconomy.gov&apos;,            &apos;Upgrade-Insecure-Requests&apos;: &apos;1&apos;,            &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.162 Safari/537.36}&apos;            }    request = requests.get(url=url, headers=header)    # 指定编码格式    request.encoding = &apos;utf-8&apos;    &apos;&apos;&apos;        发现不用指定re.S亦可以完成匹配，用了反而会报错...        注意每次匹配到的数据之后还有索引要记得标明    &apos;&apos;&apos;    link = re.findall(r&apos;&lt;a href=&quot;(.*?)&quot;&gt;Developer Tools&lt;/a&gt;&apos;, request.text)[0]    return link&apos;&apos;&apos;    second_url方法是用来获取下载文件的地址    方法也是运用了正则表达式                                            &apos;&apos;&apos;def second_url():    # 进入下载资源的页面    se_url = &apos;https://www.fueleconomy.gov&apos; + first_url()    # 添加代理,基本上的代理信息没什么变化    header_2 = {            &apos;Accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&apos;,            &apos;Accept-Encoding&apos;: &apos;gzip, deflate, br&apos;,            &apos;Accept-Language&apos;: &apos;zh-CN,zh&apos;,            &apos;Cache-Control&apos;: &apos;max-age=0&apos;,            &apos;Connection&apos;: &apos;keep-alive&apos;,            &apos;Host&apos;: &apos;www.fueleconomy.gov&apos;,            &apos;Upgrade-Insecure-Requests&apos;: &apos;1&apos;,            &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.162 Safari/537.36}&apos;            }    request_2 = requests.get(url=se_url, headers=header_2)    request_2.encoding = &apos;utf-8&apos;    # 匹配正则表达式    link_2 = re.findall(r&apos;&lt;a href=&quot;(.*?)&quot;&gt;CSV&lt;/a&gt;&apos;, request_2.text)[0]    return link_2&apos;&apos;&apos;    已经得到了资源下载地址，在主函数中进行下载并且进行解压                                                    &apos;&apos;&apos;if __name__ == &apos;__main__&apos;:    # 获取下载文件的资源地址    down_url = &apos;https://www.fueleconomy.gov&apos; + second_url()    print(&apos;数据采集完成...&apos;)    print(&apos;开始下载文件...&apos;)    # 文件名称是：vehicles.csv.zip，格式也是非常的重要！    urlretrieve(url=down_url, filename=&apos;vehicles.csv.zip&apos;)    print(&apos;下载完成！可在本工程目录查收！&apos;)</code></pre><p>至此可在本目录下查看到下载好的压缩文件：</p><p><img src="" alt=""></p><blockquote><p>当然也可加上运用<em>python</em>解压这一部分的模块，有兴趣的朋友可自行学习运用。</p></blockquote><h2 id="代码可视化分析阶段"><a href="#代码可视化分析阶段" class="headerlink" title="代码可视化分析阶段"></a>代码可视化分析阶段</h2><p>在解压之后可先用<em>excel</em>打开来看看（会发现有<strong>39000+行</strong>数据），在此只需要知道其中的参数<code>year</code>与<code>com08</code>，前者为年份，后者为燃料的使用情况相关的数值。</p><p>可视化实现代码在下：</p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>这是一个平时做的小项目，适合新手入门。</p><blockquote><p>另外，以上代码中不懂的模块知识，我建议你利用好搜索引擎，查看相关的文档或者是找本书看看。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;分析的是美国一个网站统计的多年汽车使用燃料情况的数据，需要从网上下载，在此利用好&lt;em&gt;python爬虫&lt;/em&gt;大有裨益，可作为实战运行分析。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;此篇文章案例来源于：&lt;img src=&quot;&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;本书文章中用&lt;
      
    
    </summary>
    
      <category term="可视化 爬虫" scheme="https://liujunjie11.github.io/categories/%E5%8F%AF%E8%A7%86%E5%8C%96-%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="可视化 爬虫" scheme="https://liujunjie11.github.io/tags/%E5%8F%AF%E8%A7%86%E5%8C%96-%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>关于anaconda2与anaconda3两个版本的共存问题</title>
    <link href="https://liujunjie11.github.io/2018/03/21/%E5%85%B3%E4%BA%8Eanaconda2%E4%B8%8Eanaconda3%E4%B8%A4%E4%B8%AA%E7%89%88%E6%9C%AC%E7%9A%84%E5%85%B1%E5%AD%98%E9%97%AE%E9%A2%98/"/>
    <id>https://liujunjie11.github.io/2018/03/21/关于anaconda2与anaconda3两个版本的共存问题/</id>
    <published>2018-03-21T03:03:01.000Z</published>
    <updated>2018-03-22T08:11:50.294Z</updated>
    
    <content type="html"><![CDATA[<p>最近因为用<em>anaconda</em>的关系，下载的包导致了冲突，编译器总是识别不了，就把以前<em>Mac</em>上的<em>anaconda</em>都卸载了。打算重新来过一遍解决两者（即<em>anaconda2</em>与<em>anaconda3</em>）的共存问题。</p><blockquote><p>简单说明一下<em>anaconda3</em>对应<em>python3</em>，<em>anaconda2</em>对应<em>python2</em></p></blockquote><hr><p>马上开始吧。</p><p>先是在<a href="https://www.anaconda.com/download/#macos" target="_blank" rel="external">官网上</a>下载了两个版本，如下：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-21%20%E4%B8%8A%E5%8D%8811.25.39.png" alt=""></p><blockquote><p>一路确定，直到安装完成为止。<strong>不过要说明一下，安装应该分好顺序，最后安装好的即为系统默认的了（即当我们在终端输入命令：<code>python</code> 时会出现最后安装好的那个目录中的<em>python</em>版本，我是最后安装的anaconda3）。</strong>如下图所示：</p></blockquote><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-21%20%E4%B8%8A%E5%8D%8811.29.20.png" alt=""></p><blockquote><p>Python 3.6.4 |Anaconda, Inc.| <strong>即为anaconda3为系统默认的了。</strong></p></blockquote><p>当我们再输入命令：<code>python2</code> 时与输入命令： <code>python3</code> 时，如下图所示均成为了系统默认的两个版本了。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-21%20%E4%B8%8A%E5%8D%8811.33.55.png" alt=""></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-21%20%E4%B8%8A%E5%8D%8811.34.06.png" alt=""></p><blockquote><p>这样在下载包时就能方便多了，<strong>可以直接在终端输入命令：<code>pip install ..</code> 就可以达到anaconda3对应的python3版本的包下载问题了（经过测试，已通过！）。</strong>可能有朋友疑惑，<em>python3</em>不是对应着命令<em>pip3</em>吗，实际上经过我的发现，<em>anaconda</em>的<em>python2</em>与<em>python3</em>对应的都是命令<em>pip</em>。</p></blockquote><p>虽然解决了<em>anaconda3</em>下的<em>python3</em>的问题，<strong>那么我们必须记得我们的目标是：anaconda3与anaconda2的切换使用问题。不过遗憾的是没有找到解决方案。不过倒是有一个能在同一个版本内（即anaconda3/anaconda2）同时安装两个python版本（2/3）的方案。</strong></p><p>在<a href="https://conda.io/docs/user-guide/getting-started.html#managing-envs" target="_blank" rel="external">此官网教程中</a>说明了命令。</p><ul><li>详细的命令过程可参考此篇文章：<a href="https://foofish.net/compatible-py2-and-py3.html" target="_blank" rel="external">https://foofish.net/compatible-py2-and-py3.html</a></li></ul><blockquote><p>当然参考官网亦可。</p></blockquote><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>虽然在同一版本中解决了两个<em>python</em>版本的虚拟环境的问题（可在对应的目录中找到，并且此虚拟环境均可正常使用），但是我们的问题依旧没有得到解决（即anaconda3与anaconda2如何切换使用问题），我试想用指定目录的方法运行命令，但是一无所获。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-21%20%E4%B8%8B%E5%8D%8812.43.20.png" alt=""></p><blockquote><p>Anaconda2 includes Python 2.7 and Anaconda3 includes Python 3.6. However, it does not matter which one you download, because you can create new environments that include any version of Python packaged with conda.</p><p>官网的解释已经说明了，建立虚拟环境只是解决在同一个<em>anaconda</em>版本下使用不同版本的<em>python</em>而已。所以我们想的关于anaconda2与anaconda3两个版本的共存切换问题目前或许没有办法实现。</p></blockquote><hr><p><strong>已经知道了解决方案：那就是打开各个的客户端进行下载即可！！！太简单了！！</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近因为用&lt;em&gt;anaconda&lt;/em&gt;的关系，下载的包导致了冲突，编译器总是识别不了，就把以前&lt;em&gt;Mac&lt;/em&gt;上的&lt;em&gt;anaconda&lt;/em&gt;都卸载了。打算重新来过一遍解决两者（即&lt;em&gt;anaconda2&lt;/em&gt;与&lt;em&gt;anaconda3&lt;/em&gt;
      
    
    </summary>
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>解决python文件读取时的UnicodeDecodeError: &#39;ascii&#39; codec can&#39;t decode byte 0xd4 in position 904: ordinal not in range(128)问题</title>
    <link href="https://liujunjie11.github.io/2018/03/20/%E8%A7%A3%E5%86%B3python%E6%96%87%E4%BB%B6%E8%AF%BB%E5%8F%96%E6%97%B6%E7%9A%84UnicodeDecodeError-ascii-codec-can-t-decode-byte-0xd4-in-position-904-ordinal-not-in-range-128-%E9%97%AE%E9%A2%98/"/>
    <id>https://liujunjie11.github.io/2018/03/20/解决python文件读取时的UnicodeDecodeError-ascii-codec-can-t-decode-byte-0xd4-in-position-904-ordinal-not-in-range-128-问题/</id>
    <published>2018-03-20T13:54:55.000Z</published>
    <updated>2018-03-20T14:01:51.630Z</updated>
    
    <content type="html"><![CDATA[<p>最近在处理一个<em>csv格式</em>的数据时，出现了<em>UnicodeDecodeError: ‘ascii’ codec can’t decode byte 0xd4 in<br>  position 904: ordinal not in range(128)</em>的错误告知。</p><p>查了一下，发现只要指定<strong><em>encoding=’iso-8859-1’</em></strong>即可解决此问题了。</p><p>代码演示：</p><pre><code>.....        with open(filename, encoding=&apos;iso-8859-1&apos;) as f:        ......</code></pre><hr><p>简单说明一下为何如此指定编码格式：</p><p><strong>ISO 8859-1，正式编号为ISO/IEC 8859-1:1998，又称Latin-1或“西欧语言”，是国际标准化组织内ISO/IEC 8859的第一个8位字符集。它以ASCII为基础，在空置的0xA0-0xFF的范围内，即解决0xA0-0xFF的范围内的编码错误问题，如上我们的0xd4在此范围之内。</strong></p><blockquote><p>详细可见维基百科地址：<a href="https://zh.wikipedia.org/wiki/ISO/IEC_8859-1" target="_blank" rel="external">https://zh.wikipedia.org/wiki/ISO/IEC_8859-1</a></p></blockquote><ul><li>参考：<a href="http://blog.csdn.net/programmer_wei/article/details/50994318" target="_blank" rel="external">http://blog.csdn.net/programmer_wei/article/details/50994318</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近在处理一个&lt;em&gt;csv格式&lt;/em&gt;的数据时，出现了&lt;em&gt;UnicodeDecodeError: ‘ascii’ codec can’t decode byte 0xd4 in&lt;br&gt;  position 904: ordinal not in range(128)
      
    
    </summary>
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>理解多线程编程</title>
    <link href="https://liujunjie11.github.io/2018/03/20/%E7%90%86%E8%A7%A3%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8B/"/>
    <id>https://liujunjie11.github.io/2018/03/20/理解多线程编程/</id>
    <published>2018-03-20T08:37:17.000Z</published>
    <updated>2018-03-20T08:58:49.769Z</updated>
    
    <content type="html"><![CDATA[<p>最近想研究研究什么是<em>多线程编程</em>的问题。简单了解了一下之后发现原来我对其的理解都错了…把<em>面对对象编程</em>活生生的说成了<em>多线程</em>…唉，从前文中的”口误“就…</p><h2 id="多线程与多进程"><a href="#多线程与多进程" class="headerlink" title="多线程与多进程"></a>多线程与多进程</h2><p>简单理解这两个一下：</p><ul><li><p><strong>多进程：可先理解为一个应用程序，如我们的上网用的浏览器。</strong></p></li><li><p><strong>多线程：多线程就是我们浏览器中的各种小工具，如刷新功能与新开一个标签页的功能就是两个在其中的线程。</strong></p></li></ul><blockquote><p><strong>多个进程是分开的两个应用程序，就像QQ和微信两者就是毫不相干的两个应用程序。多线程就是两个应用程序之中的多个可同时运用的小工具。</strong></p></blockquote><h2 id="多线程编程"><a href="#多线程编程" class="headerlink" title="多线程编程"></a>多线程编程</h2><p>再来看看<em>多线程编程</em>。</p><p>平时我们写程序，会将整个代码构建出各种方法函数，各有各的实现意图，以便在主类当中直接调用，最终实现目标整体意愿。</p><blockquote><p><strong>而多线程编程就是可同时在主类中调用两个或者是多个实现意图不同的方法函数，并且一同被编译器/解释器运行出结果。就成了一个多线程的问题…多线程多线程，就是能将两个或者是多个不同的方法同时运行出结果。</strong>至于是编译器还是解释器得分是什么语言了。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近想研究研究什么是&lt;em&gt;多线程编程&lt;/em&gt;的问题。简单了解了一下之后发现原来我对其的理解都错了…把&lt;em&gt;面对对象编程&lt;/em&gt;活生生的说成了&lt;em&gt;多线程&lt;/em&gt;…唉，从前文中的”口误“就…&lt;/p&gt;
&lt;h2 id=&quot;多线程与多进程&quot;&gt;&lt;a href=&quot;#多线程与多
      
    
    </summary>
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>用python/R可视化GitHub上的java热门开源项目</title>
    <link href="https://liujunjie11.github.io/2018/03/19/%E7%94%A8python%E5%8F%AF%E8%A7%86%E5%8C%96GitHub%E4%B8%8A%E7%9A%84java%E7%83%AD%E9%97%A8%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE/"/>
    <id>https://liujunjie11.github.io/2018/03/19/用python可视化GitHub上的java热门开源项目/</id>
    <published>2018-03-19T08:36:14.000Z</published>
    <updated>2018-04-03T12:41:40.170Z</updated>
    
    <content type="html"><![CDATA[<p>直接开始这个小项目吧。</p><ul><li>网页地址：<a href="https://api.github.com/search/repositories?q=language:java&amp;sort=stars" target="_blank" rel="external">https://api.github.com/search/repositories?q=language:java&amp;sort=stars</a></li></ul><blockquote><p>在这个网页中有相关的目前比较热门的开源项目（以<em>star</em>的数目来衡量），打开发现这是典型的<em>json</em>格式啊。</p></blockquote><h2 id="简单分析"><a href="#简单分析" class="headerlink" title="简单分析"></a>简单分析</h2><p>经过抓包可发现：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-19%20%E4%B8%8B%E5%8D%889.31.32.png" alt=""></p><p>即：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-19%20%E4%B8%8B%E5%8D%889.29.37.png" alt=""></p><blockquote><p>换一换就可知道所有编程语言目前比较热门的开源项目了。</p></blockquote><h2 id="python代码实现"><a href="#python代码实现" class="headerlink" title="python代码实现"></a>python代码实现</h2><pre><code>&apos;&apos;&apos;    函数目标：    将GitHub上的java热门的开源项目可视化    编写时间：    2018-3-19&apos;&apos;&apos;import requestsfrom matplotlib import pyplot as pltimport pygalfrom pygal.style import LightColorizedStyle as lcs, LightenStyle as lsif __name__ == &apos;__main__&apos;:    #添加代理配置    url = &apos;https://api.github.com/search/repositories&apos;    header = {&apos;User-Agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.162 Safari/537.36&apos;,              &apos;Connection&apos;: &apos;keep-alive&apos;}    paramter = {&apos;q&apos;: &apos;language:java&apos;,              &apos;sort&apos;: &apos;stars&apos;}    re = requests.get(url=url, params=paramter, headers=header)    # 将网页转化为python字典即用json()函数方法才可显示与网页内容一致！    re.encoding = &apos;utf-8&apos;    js_cont = re.json()    item = js_cont[&apos;items&apos;]    star_count = []    names = []    full_names = []    for each in item:            star_count.append(each[&apos;stargazers_count&apos;])            names.append(each[&apos;name&apos;])            full_names.append(each[&apos;full_name&apos;])    # 添加高亮颜色    my_style = ls(&apos;#333366&apos;, base_style=lcs)    # 添加相关的设置    my_config = pygal.Config()    my_config.label_font_size = 28    bar_chart = pygal.Bar(config=my_config, style=my_style, x_label_rotation=60, show_legend=False)    bar_chart.add(&apos;&apos;, star_count)    bar_chart.title = &apos;Java  projects stars on Github&apos;    bar_chart.x_labels = names    # 保存至目录下的文件中    bar_chart.render_to_file(&apos;Java stars in Github.svg&apos;)</code></pre><blockquote><p>之中不懂的可利用好搜索引擎。有一些爬虫的知识。</p></blockquote><p>运行得到：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-04-03%20%E4%B8%8B%E5%8D%888.36.33.png" alt=""></p><blockquote><p>简单说说用<em>python</em>可视化的感受，比较喜欢用<em>python</em>，用的比较多，意味发现<em>pygal</em>这个库做的图很漂亮。</p></blockquote><h2 id="R代码实现"><a href="#R代码实现" class="headerlink" title="R代码实现"></a>R代码实现</h2><ul><li>说明：因为<em>R</em>的爬虫没怎么看，先用可视化…日后有时间爬虫写上…</li></ul><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><ul><li>参考：《python入门与实践》【美】Eric Matthes著</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;直接开始这个小项目吧。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;网页地址：&lt;a href=&quot;https://api.github.com/search/repositories?q=language:java&amp;amp;sort=stars&quot; target=&quot;_blank&quot; rel=&quot;ex
      
    
    </summary>
    
      <category term="可视化" scheme="https://liujunjie11.github.io/categories/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
    
      <category term="python/R可视化" scheme="https://liujunjie11.github.io/tags/python-R%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>计算机基础知识（三）：带宽单位换算与存储单位换算</title>
    <link href="https://liujunjie11.github.io/2018/03/15/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%B8%A6%E5%AE%BD%E5%8D%95%E4%BD%8D%E6%8D%A2%E7%AE%97%E4%B8%8E%E5%AD%98%E5%82%A8%E5%8D%95%E4%BD%8D%E6%8D%A2%E7%AE%97/"/>
    <id>https://liujunjie11.github.io/2018/03/15/计算机基础知识（三）：带宽单位换算与存储单位换算/</id>
    <published>2018-03-15T14:06:15.000Z</published>
    <updated>2018-03-15T14:25:50.120Z</updated>
    
    <content type="html"><![CDATA[<p>位/比特（bit/b）：内存中最小的单位，二进制数序列中的一个0或一个1就是一比比特.</p><blockquote><p>1比特 = 一个二进制位，只有0和1两种状态<br>  1字节 = 8 比特</p></blockquote><p>1 Byte(B)＝8bit（位）<br>1KB＝1024Byte（字节）</p><ul><li>再来看看平时常见的下载参数：</li></ul><p><strong>Mbps：</strong>带宽单位，在 Mbps 单位中的“b”是指“Bit（位）</p><p><strong>MB/s：</strong>速度单位，其中的 B 是指“Byte（字节）</p><blockquote><p>其中1MB/s=8Mbps，下载工具一般以Bps计算，所以它们之间有8bit=1Byte的换算关系，一个字节，是由八位二进制位组成的，所以可解释一个200M的网，换算为字节，实际上仅仅极限速度能达到200/8=25M的速度。</p></blockquote><h2 id="存储单位的换算"><a href="#存储单位的换算" class="headerlink" title="存储单位的换算"></a>存储单位的换算</h2><p>计算机存储单位一般用bit、B、KB、MB、GB、TB、PB、EB、ZB、YB、BB、NB、DB……来表示，它们之间的关系是：<br>位 bit (比特)(Binary Digits)：存放一位二进制数，即 0 或 1，最小的存储单位。</p><p><strong>换算：</strong></p><p>字节byte：8个二进制位为一个字节(B)，最常用的单位。</p><p>1 Byte（B） = 8 bit</p><p>1 Kilo Byte（KB） = 1024B</p><p>1 Mega Byte（MB） = 1024 KB</p><p>1 Giga Byte （GB）= 1024 MB</p><p>1 Tera Byte（TB）= 1024 GB</p><p>1 Peta Byte（PB） = 1024 TB</p><p>1 Exa Byte（EB） = 1024 PB</p><p>1 Zetta Byte（ZB） = 1024 EB</p><p>1Yotta Byte（YB）= 1024 ZB</p><p>1 Bronto Byte（BB） = 1024 YB</p><p>1Nona Byte（NB）=1024 BB</p><p>1 Dogga Byte（DB）=1024 NB</p><p>1 Corydon Byte（CB）=1024DB</p><p>1 Xero Byte （XB）=1024CB</p><blockquote><p>进制单位全称及译音：</p><p>yotta，[尧]它， Y. 10^24，</p><p>zetta，[泽]它， Z. 10^21，</p><p>exa，[艾]可萨， E. 10^18，</p><p>peta，[拍]它， P. 10^15，</p><p>tera，[太]拉， T. 10^12，</p><p>giga，[吉]咖， G. 10^9，</p><p>mega，[兆]，M. 10^6</p></blockquote><ul><li><strong>b(bit)与B的认识</strong></li></ul><p>字节(B)是电脑中表示信息含义的最小单位，通常情况下一个ACSII码就是一个字节的空间来存放。而事实上电脑中还有比字节更小的单位，因为一个字节是由八个二进制位组成的，换一句话说，每个二进制位所占的空间才是电脑中最小的单位，我们把它称为位，也称比特（bit）。一个字节等于八位。人们之所以把字节称为电脑中表示信息含义的最小单位，是因为一位并不能表示我们现实生活中的一个相对完整的信息。</p><ul><li><strong>计算机储存单位的进率是1024而不是1000？</strong></li></ul><p>目前计算机都是二进制的，让它们计算单位，只有2的整数幂时才能非常方便计算机计算，因为电脑内部的电路工作有高电平和低电平两种状态.所以就用二进制来表示信号，(控制信号和数据)，以便计算机识别。而人习惯于使用10进制，所以存储器厂商们才用1000作进率。这样导致的后果就是实际容量要比标称容量少，不过这是合法的。1024是2的10次方，因为如果取大了，不接近10的整数次方，不方便人们计算；取小了，进率太低，单位要更多才能满足需求，所以取2的10次方正好。<br>计算实例：标称100GB的硬盘，其实际容量为100×1000×1000×1000字节/1024×1024×1024≈93.1GB<br>可见产品容量缩水只要满足计算的实际容量结果（上下误差应该在10%内）。</p><ul><li>参考：</li></ul><p><a href="https://baike.baidu.com/item/存储单位/3943356?fromtitle=计算机存储单位&amp;fromid=795305" target="_blank" rel="external">https://baike.baidu.com/item/存储单位/3943356?fromtitle=计算机存储单位&amp;fromid=795305</a></p><p><a href="https://www.jianshu.com/p/2b57116c27de" target="_blank" rel="external">https://www.jianshu.com/p/2b57116c27de</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;位/比特（bit/b）：内存中最小的单位，二进制数序列中的一个0或一个1就是一比比特.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;1比特 = 一个二进制位，只有0和1两种状态&lt;br&gt;  1字节 = 8 比特&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;1 Byte(B)＝8bi
      
    
    </summary>
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>计算机基础知识（二）：单核处理器、多核处理器、多处理器与多线程编程</title>
    <link href="https://liujunjie11.github.io/2018/03/15/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%8D%95%E6%A0%B8%E5%A4%84%E7%90%86%E5%99%A8%E3%80%81%E5%A4%9A%E6%A0%B8%E5%A4%84%E7%90%86%E5%99%A8%E3%80%81%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E4%B8%8E%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8B/"/>
    <id>https://liujunjie11.github.io/2018/03/15/计算机基础知识（二）：单核处理器、多核处理器、多处理器与多线程编程/</id>
    <published>2018-03-15T13:58:48.000Z</published>
    <updated>2018-03-15T14:03:52.921Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>转载自：<a href="http://blog.csdn.net/zolalad/article/details/28393209" target="_blank" rel="external">http://blog.csdn.net/zolalad/article/details/28393209</a></p></blockquote><p><strong>一．进程、线程、单核处理器</strong></p><p>进程和线程都是操作系统的概念。进程是应用程序的执行实例，每个进程是由私有的虚拟地址空间、代码、数据和其它各种系统资源组成，即进程是操作系统进行资源分配的最小单元。进程在运行过程中创建的资源随着进程的终止而被销毁，所使用的系统资源在进程终止时被释放或关闭。</p><p>线程是进程内部的一个执行单元。系统创建好进程后，实际上就启动执行了该进程的主执行线程，主执行线程以函数地址形式，比如说main或WinMain函数，将程序的启动点提供给Windows系统。主执行线程终止了，进程也就随之终止。</p><p>每一个进程至少有一个主执行线程，它无需由用户去主动创建，是由系统自动创建的。用户根据需要在应用程序中创建其它线程，多个线程并发地运行于同一个进程中。一个进程中的所有线程都在该进程的虚拟地址空间中，共同使用这些虚拟地址空间、全局变量和系统资源，所以线程间的通讯非常方便，多线程技术的应用也较为广泛。</p><p>多线程可以实现并行处理，避免了某项任务长时间占用CPU时间。要说明的一点是，目前大多数的操作系统教材中的单处理器都是指的单核处理器。对于单核单处理器（CPU）的，为了运行所有这些线程，操作系统为每个独立线程安排一些CPU时间，操作系统以轮换方式向线程提供时间片，这就给人一种假象，好象这些线程都在同时运行。由此可见，如果两个非常活跃的线程为了抢夺对CPU的控制权，在线程切换时会消耗很多的CPU资源，反而会降低系统的性能。</p><p>最开始，线程只是用于分配单个处理器的处理时间的一种工具。但假如操作系统本身支持多个处理器，那么每个线程都可分配给一个不同的处理器，真正进入“并行运算”状态。从程序设计语言的角度看，多线程操作最有价值的特性之一就是程序员不必关心到底使用了多少个处理器，程序员只需将程序编写成多线程模式即可。程序在逻辑意义上被分割为数个线程；假如机器本身安装了多个处理器，那么程序会运行得更快，毋需作出任何特殊的调校。根据前面的论述，大家可能感觉线程处理非常简单。但必须注意一个问题：共享资源！如果有多个线程同时运行，而且它们试图访问相同的资源，就会遇到一个问题。举个例子来说，两个线程不能将信息同时发送给一台打印机。为解决这个问题，对那些可共享的资源来说（比如打印机），它们在使用期间必须进入锁定状态。所以一个线程可将资源锁定，在完成了它的任务后，再解开（释放）这个锁，使其他线程可以接着使用同样的资源。</p><p><strong>多线程是为了同步完成多项任务，不是为了提高运行效率，而是为了提高资源使用效率来提高系统的效率。线程是在同一时间需要完成多项任务的时候实现的。</strong></p><p>最简单的比喻多线程就像火车的每一节车厢，而进程则是火车。车厢离开火车是无法跑动的，同理火车也不可能只有一节车厢。多线程的出现就是为了提高效率。同时它的出现也带来了一些问题。</p><p><strong>注</strong>：单核处理器并不是一个长久以来存在的概念，在近年来多核心处理器逐步普及之后，单核心的处理器为了与双核和四核对应而提出。顾名思义处理器只有一个逻辑核心。</p><p><strong>二、多核处理器和多处理器的区别</strong></p><p>多核是指一个CPU有多个核心处理器，处理器之间通过CPU内部总线进行通讯。而多CPU是指简单的多个CPU工作在同一个系统上，多个CPU之间的通讯是通过主板上的总线进行的。从以上原理可知，N个核的CPU，要比N个CPU在一起的工作效率要高（单核性能一致的情况下）。</p><p><strong>三、 处理器结构对并发程序的影响</strong></p><p>对称多处理器是最主要的多核处理器架构。在这种架构中所有的CPU共享一条系统总线（BUS）来连接主存。而每一个核又有自己的一级缓存，相对于BUS对称分布[2]，如下图：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/20140509162415500.jpeg" alt=""></p><p>这种架构在并发程序设计中，大致会引来两个问题，一个是内存可见性，一个是Cache一致性流量。内存可见性属于并发安全的问题，Cache一致性流量引起的是性能上的问题。</p><p><strong>内存可见性</strong>：内存可见性在单处理器或单线程情况下是不会发生的。在一个单线程环境中，一个变量选写入值，然后在没有干涉的情况下读取这个变量，得到的值应该是修改过的值。但是在读和写不在同一个线程中的时候，情况却是不可以预料的。Core1和Core2可能会同时把主存中某个位置的值Load到自己的一级缓存中，而Core1修改了自己一级缓存中的值后，却不更新主存中的值，这样对于Core2来讲，永远看不到Core1对值的修改。在Java程序设计中，用锁，关键字volatile，CAS原子操作可以保证内存可见。</p><p><strong>Cache一致性问题</strong>：指的是在SMP结构中，Core1和Core2同时下载了主存中的值到自己的一级缓存中，Core1修改了值后，会通过总线让Core2中的值失效，Core2发现自己存的值失效后，会再通过总线从主存中得到新值。总线的通信能力是固定的，通过总线使各CPU的一级缓存值数据同步的流量过大，那么总线就会成瓶颈。这种影响属于性能上的影响，减小同步竞争就能减少一致性流量。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;转载自：&lt;a href=&quot;http://blog.csdn.net/zolalad/article/details/28393209&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://blog.csdn.net/zola
      
    
    </summary>
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="学习笔记" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>用python3爬取QQ音乐列表音乐</title>
    <link href="https://liujunjie11.github.io/2018/03/14/%E7%94%A8python3%E7%88%AC%E5%8F%96QQ%E9%9F%B3%E4%B9%90%E5%88%97%E8%A1%A8%E9%9F%B3%E4%B9%90/"/>
    <id>https://liujunjie11.github.io/2018/03/14/用python3爬取QQ音乐列表音乐/</id>
    <published>2018-03-14T12:57:31.000Z</published>
    <updated>2018-03-15T08:48:37.038Z</updated>
    
    <content type="html"><![CDATA[<p>最近想爬取一些音乐来实战一下，选择了<em>qq音乐</em>。</p><p><em>qq音乐</em>明显的就是一个动态网页，所以需要抓包了。</p><blockquote><p>不懂的关键词可利用好搜索引擎。</p></blockquote><h2 id="分析过程"><a href="#分析过程" class="headerlink" title="分析过程"></a>分析过程</h2><p>在此就说说分析的大致过程吧。</p><p>先看看主页：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8A%E5%8D%8811.08.29.png" alt=""></p><p>我们随便点开一个主题列表：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8A%E5%8D%8811.09.44.png" alt=""></p><p>因为是动态网页，所以就在这里抓包吧，因为<em>qq音乐</em>是动态网页，需要相关的参数信息才能得到想要的音乐地址，随便以播放一首歌曲为例，如下图1中的歌曲<em>ID</em>，点进去这个看看，即点击播放按钮，发现来到了播放页面，打开我用的<em>Chrome</em>中的开发者工具，里面有我们想要的音乐地址（如下图2所示），图3展示得到列表歌曲的所有信息，需要编程清洗之后才能得到我们想要，在此会在下面的代码中标明。</p><ul><li>图1:</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8A%E5%8D%8811.37.52.png" alt=""></p><ul><li>图2:</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8A%E5%8D%8811.41.18.png" alt=""></p><ul><li>图3:</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8A%E5%8D%8811.51.03.png" alt=""></p><p>其中的<em>URL</em>地址，代码中会用到：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8B%E5%8D%8812.48.57.png" alt=""></p><blockquote><p>现在我们可能就会有思路出现了：抓包爬取列表的所有歌曲的<em>ID号</em>以及歌曲信息 <strong>–&gt;</strong> 整合到以<em>ID</em>为基的<em>html</em>地址 <strong>–&gt;</strong> 到播放页面利用<em>beautiful模块</em>爬取相关的音乐地址即可！so easy~</p></blockquote><p><strong>但经过我的测试说明，爬取播放页面的<em>html</em>信息是没有相关的音乐地址的，所以在得到歌曲<em>ID</em>信息，整合到以<em>ID</em>为基的<em>html</em>地址之后，我们还需要对播放页面进行抓包。下面说说如何在播放页面抓包。</strong></p><p>先播放一首歌曲，再进行抓包，发现了这些信息（如下图1），再结合上面的音乐地址分析一下，发现了<em>vkey</em>信息的存在，即每一首歌的<em>vkey</em>信息是不同的，并且经过测试即便是同一个<em>ID</em>，<em>vkey</em>也是一直不断自动变换着的，不过在测试之后可得出结论：<strong>只要得到<em>vkey</em>信息，再整合上面的音乐地址就能抓到音乐信息，并且经过代码编译之后下载下来。</strong></p><blockquote><p>特别说明一下，<strong>下载歌曲的地址以及我们抓包时的<em>URL</em>之中，仅仅有如<em>vkey</em>的不同，或者是一些<em>ID</em>的不同，其他的参数是相同的！</strong>所以我们才能仅仅抓到<em>vkey</em>信息就能方便的下载歌曲，就是这么个意思。不理解的朋友可细心观察一下。</p></blockquote><ul><li>图1:</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8A%E5%8D%8811.55.11.png" alt=""></p><ul><li>图2:</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8A%E5%8D%8811.58.13.png" alt=""></p><p>其中的<em>URL</em>地址：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8B%E5%8D%8812.49.29.png" alt=""></p><hr><h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><pre><code>&apos;&apos;&apos;函数目标：爬取QQ音乐列表音乐编写时间：2018-3-15&apos;&apos;&apos;import requestsimport os import timeimport json from urllib.request import urlretrieveif __name__ == &apos;__main__&apos;:    # 建立目录用于装爬取的音乐    if &apos;QQ音乐列表音乐&apos; not in os.listdir():        os.makedirs(&apos;QQ音乐列表音乐&apos;)    &apos;&apos;&apos;        从URL中添加代理记忆必要的相关的参数以获取歌曲的ID以及歌曲名                                                            &apos;&apos;&apos;    playlist_url = &apos;https://c.y.qq.com/qzone/fcg-bin/fcg_ucc_getcdinfo_byids_cp.fcg&apos;    # 添加页面中的代理信息    header = {&apos;user-agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos;,            &apos;cookie&apos;:&apos;tvfe_boss_uuid=1770396a4ed2d111; pgv_info=ssid=s4189101616; pgv_pvid=7344469728; uin=; pgv_pvi=8737627136; pgv_si=s9378960384; _tucao_userinfo=ZU1hSHhlWVNPSnRoNWgwTjMzc2c4OVYzYjBuTkNjcDNHNjcyVkkzWm9WUkJZMWhxWDJ5SmpTSURFWDhVTk9TYkczU3JWc09EeEsrMEVnQ2RpK2FVNWh4M0x0Y21aOG5Vcms5MW9odmt6ZXNxRWlmeE9PZzM0SlQ1YmVuM0xhRVpUaWh5d2REV2FZcHdQdWdNL0daWE9rTTM0MlFjc1VoaHhNVkh4bkNqbnBOMDN2MG1sOEkxc0dYNFZRa24rd0RY--FeceGG8ErqgRGZz7WWwpsg%3D%3D; _tucao_session=WUVSc2RVVk95Y0ViU2NoNndsWmVlbzZoSG1WaFdpcEk4Q1M5bXZSTG9qanV3OEpuNVNQT3dBc0tBRERUY1NCRDZJek14Y2xYeFdmMWhiaWdkZ282UjdPdXVyT1ZYQnpCeG9BcklQUFBEMU5LQ3F3ajdmd3VWRmZ5QTJoN1ViS1krcEx0aUdUb3plckVNVGc3K0t2Z3pUeFJDcFZMNnU3dEpLUXZ5Zyt4dUpJdU5Hb3ZwZUhpTHM0OEhNQk0vcHJKN2tEOXVZay95WkFpZlFuSVBQZDhoSzlMVUMrVDQxN0llRzJuNkVWUGdTVjdyaVl2WVdscFlyVDJPald4MG9BWA%3D%3D--dDBBK5gXjLaGccOBzx4EBA%3D%3D; ts_refer=www.google.com/; ts_uid=3146042580; qqmusic_fromtag=66; yq_playdata=s; yqq_stat=0; yplayer_open=1; yq_index=0; yq_playschange=0; player_exist=1&apos;,            &apos;referer&apos;:&apos;https://y.qq.com/n/yqq/playlist/3766176211.html&apos;}    # 添加参数信息,有些是非必须的，待研究，有兴趣的可以自己测试    paramter = {                &apos;type&apos;:&apos;1&apos;,                &apos;json&apos;:&apos;1&apos;,                &apos;utf8&apos;:&apos;1&apos;,                &apos;onlysong&apos;:&apos;0&apos;,                &apos;disstid&apos;:&apos;3766176211&apos;,                &apos;format&apos;:&apos;jsonp&apos;,                &apos;g_tk&apos;:&apos;5381&apos;,  # 非必须                &apos;jsonpCallback&apos;:&apos;playlistinfoCallback&apos;,  # 值可更改                &apos;loginUin&apos;:&apos;0&apos;,                &apos;hostUin&apos;:&apos;0&apos;,                &apos;format&apos;:&apos;jsonp&apos;,                &quot;inCharset&quot;:&apos;utf8&apos;,                &apos;outCharset&apos;:&apos;utf-8&apos;,                &apos;notice&apos;:&apos;0&apos;,                &apos;platform&apos;:&apos;yqq&apos;,                &apos;needNewCode&apos;:&apos;0&apos;,                }    playlist_re = requests.get(url=playlist_url, params=paramter, headers=header)    # 指定编码格式    playlist_re.encoding = &apos;utf-8&apos;    # 改变为python可识别的json格式,进行必要的数据清洗,去掉前面的&apos;jsonpCallback&apos;部分    playlist_info = json.loads(playlist_re.text.lstrip(&apos;playlistinfoCallback(&apos;).rstrip(&apos;)&apos;))    # 指定整体索引    playlist_info1 = playlist_info[&apos;cdlist&apos;][0]    # 先存储歌手的姓名,观察可知，一共有19个索引,因为歌曲本身仅仅有20首，取前20个歌手名    singer_name = []    for num in range(0, 17):        singer_eainfo = playlist_info1[&apos;songlist&apos;][num]        for each_info in singer_eainfo[&apos;singer&apos;]:            singer_name.append(each_info[&apos;name&apos;])    num = 0    # 在循环体系中进行下一步的编写    for each in playlist_info1[&apos;songlist&apos;]:        &apos;&apos;&apos;            在获取歌曲vkey的主URL传入相关的参数得到相关的数据之后进行挖掘，得到vkey信息            其中的参数有些是不必要的，可自由修改，有些是必要的                                                               &apos;&apos;&apos;        key_url = &apos;https://c.y.qq.com/base/fcgi-bin/fcg_music_express_mobile3.fcg&apos;        # 传入相关的代理以及参数        header_1 = {&apos;user-agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36&apos;,                  &apos;referer&apos;:&apos;https://y.qq.com/portal/player.html&apos;,                  &apos;cookie&apos;:&apos;tvfe_boss_uuid=1770396a4ed2d111; pgv_info=ssid=s4189101616; pgv_pvid=7344469728; uin=; pgv_pvi=8737627136; pgv_si=s9378960384; _tucao_userinfo=ZU1hSHhlWVNPSnRoNWgwTjMzc2c4OVYzYjBuTkNjcDNHNjcyVkkzWm9WUkJZMWhxWDJ5SmpTSURFWDhVTk9TYkczU3JWc09EeEsrMEVnQ2RpK2FVNWh4M0x0Y21aOG5Vcms5MW9odmt6ZXNxRWlmeE9PZzM0SlQ1YmVuM0xhRVpUaWh5d2REV2FZcHdQdWdNL0daWE9rTTM0MlFjc1VoaHhNVkh4bkNqbnBOMDN2MG1sOEkxc0dYNFZRa24rd0RY--FeceGG8ErqgRGZz7WWwpsg%3D%3D; _tucao_session=WUVSc2RVVk95Y0ViU2NoNndsWmVlbzZoSG1WaFdpcEk4Q1M5bXZSTG9qanV3OEpuNVNQT3dBc0tBRERUY1NCRDZJek14Y2xYeFdmMWhiaWdkZ282UjdPdXVyT1ZYQnpCeG9BcklQUFBEMU5LQ3F3ajdmd3VWRmZ5QTJoN1ViS1krcEx0aUdUb3plckVNVGc3K0t2Z3pUeFJDcFZMNnU3dEpLUXZ5Zyt4dUpJdU5Hb3ZwZUhpTHM0OEhNQk0vcHJKN2tEOXVZay95WkFpZlFuSVBQZDhoSzlMVUMrVDQxN0llRzJuNkVWUGdTVjdyaVl2WVdscFlyVDJPald4MG9BWA%3D%3D--dDBBK5gXjLaGccOBzx4EBA%3D%3D; ts_refer=www.google.com/; ts_uid=3146042580; qqmusic_fromtag=66; yq_playdata=s; yqq_stat=0; yq_index=0; yq_playschange=0; player_exist=1; ts_last=y.qq.com/n/yqq/playlist/3766176211.html; yplayer_open=1&apos;}            paramter_1 = {                    &apos;g_tk&apos;:&apos;5381&apos;,  # 非必须                    &apos;jsonpCallback&apos;:&apos;MusicJsonCallback&apos;,  # 非必须，可更改                    &quot;loginUin&quot;:&apos;0&apos;,                    &apos;hostUin&apos;:&apos;0&apos;,                    &apos;format&apos;:&apos;json&apos;,                    &apos;inCharset&apos;:&apos;utf8&apos;,                    &apos;outCharset&apos;:&apos;utf-8&apos;,                    &apos;notice&apos;:&apos;0&apos;,                    &apos;platform&apos;:&quot;yqq&quot;,                    &apos;needNewCode&apos;:&apos;0&apos;,                    &apos;cid&apos;:&apos;205361747&apos;,  # 一致必须                    &apos;callback&apos;:&apos;MusicJsonCallback&apos;,  # 非必须，可更改                    &apos;uin&apos;:&apos;0&apos;,                    # 传入获取的信息                    &apos;songmid&apos;:each[&apos;songmid&apos;],                    &apos;filename&apos;:&apos;C400&apos; + each[&apos;songmid&apos;] + &apos;.m4a&apos;,                    &apos;guid&apos;:&apos;7344469728&apos;                       }        # 解析得到含有vkey的数据信息，然后进行清洗得到想要的信息        key_re = requests.get(url=key_url, params=paramter_1, headers=header_1)        # 指定编码格式        key_re.encoding = &apos;utf-8&apos;        # 转换为python的json格式，进行简单的清洗        key_info = json.loads(key_re.text.lstrip(&apos;MusicJsonCallback(&apos;).rstrip(&apos;)&apos;))        # 进一步的清洗        data_info = key_info[&apos;data&apos;]        items_info = data_info[&apos;items&apos;][0]        print(&apos;数据采集完成，开始下载任务...&apos;)        # 接下来就是可以下载了        urlretrieve(url=&apos;http://dl.stream.qqmusic.qq.com/C4000041FwTv0Ai3Ku.m4a?vkey=&apos; + items_info[&apos;vkey&apos;] + &apos;&amp;guid=7344469728&amp;uin=0&amp;fromtag=66.mp3&apos;, filename=&apos;QQ音乐列表音乐/&apos; + singer_name[num] + &apos;-&apos; + each[&apos;songname&apos;] + &apos;.mp3&apos;)        print(&apos;正在下载:&apos; + singer_name[num] + &apos;的&apos; + each[&apos;songname&apos;] + &apos;!&apos;)        print(&apos;下载中....&apos;)        print(&apos;下载此歌曲完成！&apos;)        # 跳传到下一个歌手名        num = num + 1        time.sleep(1)    print(&apos;全部下载完成，请在本过程目录下查收！&apos;)</code></pre><blockquote><p><strong>在使用urlretrieve函数时，其中的url参数输入时应当加上格式，如下载视频时加上.mp4,下载音乐时加上.mp3,否则会易出现HTTP 403 错误 – 禁止访问 (Forbidden)</strong></p></blockquote><ul><li>链接：<a href="http://www.checkupdown.com/status/E403_zh.html" target="_blank" rel="external">认识HTTP 403 错误 – 禁止访问 (Forbidden)</a></li></ul><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>运行：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8B%E5%8D%884.08.01.png" alt=""></p><p>到目录查看：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-15%20%E4%B8%8B%E5%8D%884.09.17.png" alt=""></p><blockquote><p>一切还算是顺利。前段时间想爬取腾讯视频，研究了挺久，没有成功，还需要学习，腾讯的资源都在腾讯云上，我想方式都差不多。</p></blockquote><ul><li>参考：<a href="http://blog.csdn.net/lht_okk/article/details/77206510" target="_blank" rel="external">http://blog.csdn.net/lht_okk/article/details/77206510</a></li></ul><h2 id="后续说明"><a href="#后续说明" class="headerlink" title="后续说明"></a>后续说明</h2><p><strong>经过后来的测试，本代码爬取的思路还是正确的，但是爬取的信息流只能是同一个了…即便是不同的ID…</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近想爬取一些音乐来实战一下，选择了&lt;em&gt;qq音乐&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;qq音乐&lt;/em&gt;明显的就是一个动态网页，所以需要抓包了。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;不懂的关键词可利用好搜索引擎。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;分
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>用python3爬取新加坡联合早报新闻小视频</title>
    <link href="https://liujunjie11.github.io/2018/03/14/%E7%94%A8python3%E7%88%AC%E5%8F%96%E6%96%B0%E5%8A%A0%E5%9D%A1%E8%81%94%E5%90%88%E6%97%A9%E6%8A%A5%E6%96%B0%E9%97%BB%E5%B0%8F%E8%A7%86%E9%A2%91/"/>
    <id>https://liujunjie11.github.io/2018/03/14/用python3爬取新加坡联合早报新闻小视频/</id>
    <published>2018-03-14T10:58:25.000Z</published>
    <updated>2018-03-14T11:35:26.345Z</updated>
    
    <content type="html"><![CDATA[<p>位于新加坡的<a href="http://www.zaobao.com" target="_blank" rel="external">联合早报</a>是我几乎每天都会看的新闻网址，标题清晰明了，思路严谨踏实，是个好的新闻网站，值得推荐。不过却是被墙了…</p><p>今天看了这一篇文章：<a href="http://www.zaobao.com/realtime/china/story20180313-842407" target="_blank" rel="external">女记者提问冗长 人民大会堂部长通道出现“飙戏”一幕</a></p><blockquote><p>非常有意思，想收藏其中的视频，于是想到了用<em>python</em>爬取好了。</p></blockquote><p>这个新闻网站明显是一个动态网站啊，两种方式：</p><ul><li>第一种：通过抓包，如下可得知相关的<em>video</em>信息</li></ul><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.03.30.png" alt=""></p><ul><li>第二种：网站自带的连接</li></ul><p>如下操作，点击视频中<em>share</em>，可发现资源地址</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.03.57.png" alt=""></p><blockquote><p>右上角的<em>share</em>。</p></blockquote><p>其中有地址信息。</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.04.10.png" alt=""></p><p>在获取的地址前加上<em>http:</em>简单测试一下:</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.07.10.png" alt=""></p><blockquote><p>是正确的，网站真的很贴心呢～</p></blockquote><p>以刚刚的地址输入爬取下来的代码：</p><pre><code>from urllib.request import urlretrieve if __name__ == &apos;__main__&apos;:    print(&apos;开始下载...&apos;)    urlretrieve(url=&apos;http://players.brightcove.net/4802324430001/H1dr7zTWz_default/index.html?videoId=5750255765001.mp4&apos;, filename=&apos;两会小视频.mp4&apos;)    print(&apos;下载完成！&apos;)</code></pre><p><strong>结果：发现可在网上播放的视频下载之后却不能播放…占用的内存才几百kb…这一看就知道地址是错的…</strong></p><hr><p>经过上面网址播放的连接，再次进行抓包，打开相关的网页意外发现了<em>mp4格式</em>的连接：</p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.28.05.png" alt=""></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.30.37.png" alt=""></p><hr><p><strong>将此链接替换掉上面代码中的URL地址，发现可以了（如下图），完工。</strong></p><p><img src="http://owudg3xs2.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-03-14%20%E4%B8%8B%E5%8D%887.31.51.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;位于新加坡的&lt;a href=&quot;http://www.zaobao.com&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;联合早报&lt;/a&gt;是我几乎每天都会看的新闻网址，标题清晰明了，思路严谨踏实，是个好的新闻网站，值得推荐。不过却是被墙了…&lt;/p&gt;
&lt;p&gt;今
      
    
    </summary>
    
      <category term="学习" scheme="https://liujunjie11.github.io/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="学习" scheme="https://liujunjie11.github.io/tags/%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
